{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a5a326-358a-4c17-b35c-3c102d90d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "def configure_gpus():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(f\"{len(gpus)} Physical GPU(s), {len(logical_gpus)} Logical GPU(s) configured.\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError in configuring GPUs: {e}\")\n",
    "    else:\n",
    "        print(\"No GPU is available.\")\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "def check_jupyter_notebook():\n",
    "    try:\n",
    "        cfg = get_ipython().config \n",
    "        print(\"Jupyter Notebook environment detected. Configuring...\")\n",
    "        cfg.NotebookApp.iopub_msg_rate_limit = 20000.0\n",
    "        cfg.NotebookApp.rate_limit_window = 10.0\n",
    "    except NameError:\n",
    "        print(\"Not running in a Jupyter Notebook environment.\")\n",
    "\n",
    "# Clear TensorFlow session and suppress warnings\n",
    "tf.keras.backend.clear_session()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GPU configuration and seed setting\n",
    "configure_gpus()\n",
    "seed_everything()\n",
    "\n",
    "# Check if running in Jupyter Notebook and configure\n",
    "check_jupyter_notebook()\n",
    "\n",
    "# Initialize TensorFlow distributed strategy\n",
    "#strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8a1dc-fa73-4b67-9e8b-9bffbcd6d45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib #for saving model files as pkl files\n",
    "import os\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import imgaug.augmenters as iaa\n",
    "sns.set(palette='muted',style='white')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense,GlobalAveragePooling2D, Input, Embedding, LSTM,Dot,Reshape,Concatenate,BatchNormalization, GlobalMaxPooling2D, Dropout, Add\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import sentence_bleu #bleu score\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import GRU\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "import os\n",
    "import math\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90321139-9b58-49a6-983e-a26ca892da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV = 'ImageCLEFmedical_Caption_2023_caption_prediction_train_labels.csv'\n",
    "VALID_CSV = 'ImageCLEFmedical_Caption_2023_caption_prediction_valid_labels.csv'\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "df_train = pd.read_csv(TRAIN_CSV, delimiter='\\t')\n",
    "df_valid = pd.read_csv(VALID_CSV, delimiter='\\t')\n",
    "\n",
    "\n",
    "image_caption_pairs = {}\n",
    "caption_ls = []\n",
    "vocab_list = set()\n",
    "cap_len = []\n",
    "\n",
    "def process_caption(text):\n",
    "    # Lowercase the caption label\n",
    "    caption_labels_lower = text.lower()\n",
    "    caption_labels_lower = caption_labels_lower.strip()\n",
    "\n",
    "    # Replace hyphens with spaces\n",
    "    caption_labels_clean = re.sub(r'-', ' ', caption_labels_lower)\n",
    "\n",
    "    # Remove special characters using regular expressions\n",
    "    caption_labels_clean = re.sub(r'[^a-zA-Z0-9\\s]', '', caption_labels_clean)\n",
    "\n",
    "    # Tokenize the caption label into words\n",
    "    words = caption_labels_clean.split()\n",
    "\n",
    "    # Remove numbers, words containing any numeric values\n",
    "    processed_words = [word for word in words if not word.isdigit() and not any(char.isdigit() for char in word)]\n",
    "\n",
    "    # Filter out words with 4 or fewer characters\n",
    "    processed_words = [word for word in processed_words if len(word) > 4]\n",
    "    \n",
    "    # Join the processed words back into a sentence\n",
    "    caption_labels_processed = ' '.join(processed_words)\n",
    "    caption_labels_processed = '<START> '+caption_labels_processed+ ' <END>'\n",
    "    return caption_labels_processed\n",
    "\n",
    "\n",
    "# Process training images and captions\n",
    "for index, row in tqdm(df_train.iterrows(), total=df_train.shape[0], desc=\"Training Image and Caption Data\"):\n",
    "    img_id, img_caption = row['ID'], row['caption']\n",
    "    img_path = 'ImageCLEFmedical_Caption_2023_train_images/' + str(img_id) + '.jpg'\n",
    "    caption = process_caption(img_caption)\n",
    "\n",
    "    # Check if the image file exists\n",
    "    if os.path.exists(img_path):\n",
    "        # Check if img_id is already a key in caption_mapping; if not, initialize it\n",
    "        if img_id not in image_caption_pairs:\n",
    "            image_caption_pairs[img_path] = []\n",
    "\n",
    "        # Add the processed caption to the list for this image\n",
    "        image_caption_pairs[img_path].append(caption)\n",
    "        caption_ls.append(caption)\n",
    "        vocab_list.update(caption.split(' '))\n",
    "        words = caption.split(' ')\n",
    "        cap_len.append(len(words))\n",
    "    else:\n",
    "        print(f\"File not found: {img_path}\")\n",
    "\n",
    "# Process validation images and captions\n",
    "for index, row in tqdm(df_valid.iterrows(), total=df_valid.shape[0], desc=\"Validation Image and Caption Data\"):\n",
    "    img_id, img_caption = row['ID'], row['caption']\n",
    "    img_path = 'ImageCLEFmedical_Caption_2023_valid_images/' + str(img_id) + '.jpg'\n",
    "    caption = process_caption(img_caption)\n",
    "\n",
    "    # Check if the image file exists\n",
    "    if os.path.exists(img_path):\n",
    "        # Check if img_id is already a key in caption_mapping; if not, initialize it\n",
    "        if img_id not in image_caption_pairs:\n",
    "            image_caption_pairs[img_path] = []\n",
    "\n",
    "        # Add the processed caption to the list for this image\n",
    "        image_caption_pairs[img_path].append(caption)\n",
    "\n",
    "        # Additional processing (if needed)\n",
    "        caption_ls.append(caption)\n",
    "        vocab_list.update(caption.split(' '))\n",
    "        words = caption.split(' ')\n",
    "        cap_len.append(len(words))\n",
    "    else:\n",
    "        print(f\"File not found: {img_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ec5dfe-a23b-49aa-9bbb-e4ab8665f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Calculate frequencies\n",
    "frequency = Counter(cap_len)\n",
    "\n",
    "\n",
    "# Sort items by frequency in descending order\n",
    "sorted_frequency = sorted(frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Total number of items\n",
    "total_items = sum(frequency.values())\n",
    "\n",
    "# Sort items by frequency in descending order\n",
    "sorted_frequency = sorted(frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Calculate cumulative frequency and find the 90% point\n",
    "cumulative = 0\n",
    "ninety_percent_mark = total_items * 0.99\n",
    "for num, freq in sorted_frequency:\n",
    "    cumulative += freq\n",
    "    if cumulative >= ninety_percent_mark:\n",
    "        print(f\"The item at the 99% cumulative frequency mark is: {num}\")\n",
    "        break\n",
    "        \n",
    "# So the maximum length should be 85\n",
    "max_length = num\n",
    "print(\"The maximum length is:\", max_length)\n",
    "print(\"# of words in Vocabulary set:\", len(vocab_list))\n",
    "\n",
    "# Words to check\n",
    "words_to_check = [\"<START>\", \"<END>\", \"\"]\n",
    "\n",
    "# Check if the <START>, <END>, '' are in the list\n",
    "for word in words_to_check:\n",
    "    if word in vocab_list:\n",
    "        print(f\"'{word}' is in the list.\")\n",
    "    else:\n",
    "        print(f\"'{word}' is not in the list.\")\n",
    "\n",
    "# Check for empty strings\n",
    "empty_string_found = False\n",
    "for index, caption in enumerate(caption_ls):\n",
    "    if caption.strip() == \"\":  # Using strip() to also catch strings with only whitespace\n",
    "        empty_string_found = True\n",
    "        print(f\"Empty string found at index {index}\")\n",
    "\n",
    "if not empty_string_found:\n",
    "    print(\"No empty strings found in the list.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684282ec-a48f-47df-b651-9d45a75358a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('pickles_clef23/data.pkl')\n",
    "max_pad = 100\n",
    "df = data\n",
    "input_size = (224,224)\n",
    "batch_size=16\n",
    "embedding_dim = 300\n",
    "dense_dim = 512\n",
    "lstm_units = dense_dim\n",
    "dropout_rate = 0.2\n",
    "vocab_size = len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec9eb3c-f946-40f3-8c36-f85ee2de37d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Assuming 'data' is your original dictionary with image paths as keys and caption lists as values\n",
    "# and 'max_pad' is the maximum length for padding the captions\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "all_captions = [cap[0] for cap in data.values()]  # Extract all captions\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "\n",
    "# # Tokenize and pad all captions\n",
    "# tokenized_captions = tokenizer.texts_to_sequences(all_captions)\n",
    "# padded_captions = pad_sequences(tokenized_captions, maxlen=max_pad, padding='post')\n",
    "\n",
    "# # Add tokenized captions back to the data dictionary\n",
    "# for i, key in enumerate(data.keys()):\n",
    "#     data[key] = padded_captions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca098d-c7af-4bf7-bf43-37d92a791997",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_val_test_split(caption_data, train_size=0.8, val_size=0.15, shuffle=True):\n",
    "    all_images = list(caption_data.keys())\n",
    "    if shuffle:\n",
    "        np.random.shuffle(all_images)\n",
    "    total_size = len(caption_data)\n",
    "    train_end = int(total_size * train_size)\n",
    "    val_end = train_end + int(total_size * val_size)  # Calculate the end index for validation set\n",
    "    \n",
    "    # Split the data\n",
    "    training_data = {img_name: caption_data[img_name] for img_name in all_images[:train_end]}\n",
    "    validation_data = {img_name: caption_data[img_name] for img_name in all_images[train_end:val_end]}\n",
    "    test_data = {img_name: caption_data[img_name] for img_name in all_images[val_end:]}  # Remaining data for testing\n",
    "\n",
    "    return training_data, validation_data, test_data\n",
    "\n",
    "# Assuming 'data' is your dataset\n",
    "train_data, val_data, test_data = train_val_test_split(data)\n",
    "\n",
    "print(\"Number of training samples: \", len(train_data))\n",
    "print(\"Number of validation samples: \", len(val_data))\n",
    "print(\"Number of test samples: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deabd3e1-0272-4d82-8464-c52318a7bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class Dataset(Sequence):\n",
    "    def __init__(self, data, input_size, tokenizer, max_pad, augment=False):\n",
    "        self.images = list(data.keys())\n",
    "        self.captions = list(data.values())\n",
    "        self.input_size = input_size\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_pad = max_pad\n",
    "        self.augment = augment  # Flag to control augmentation\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image_path = self.images[i]\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "        image = tf.image.resize(image, self.input_size)\n",
    "\n",
    "        # Apply augmentation if enabled\n",
    "        if self.augment:\n",
    "            image = self.apply_augmentation(image)\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "        caption = self.captions[i]\n",
    "        caption_seq = self.tokenizer.texts_to_sequences(caption)\n",
    "        caption_seq = pad_sequences(caption_seq, maxlen=self.max_pad, padding='post')\n",
    "        caption_seq = tf.squeeze(caption_seq, axis=0)\n",
    "\n",
    "        # Creating the target sequence by shifting the caption\n",
    "        target_seq = np.zeros_like(caption_seq)\n",
    "        target_seq[:-1] = caption_seq[1:]\n",
    "\n",
    "        return image, caption_seq, target_seq\n",
    "\n",
    "    def apply_augmentation(self, image):\n",
    "        # Randomly flip the image horizontally\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "        # Randomly adjust brightness\n",
    "        image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "\n",
    "        # Add more augmentation methods here if needed\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "class Dataloader(Sequence):\n",
    "    def __init__(self, dataset, batch_size=1, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indexes = np.arange(len(self.dataset))\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        start = i * self.batch_size\n",
    "        stop = min((i + 1) * self.batch_size, len(self.dataset))\n",
    "        indexes = self.indexes[start:stop]\n",
    "\n",
    "        batch_images = []\n",
    "        batch_captions = []\n",
    "        batch_targets = []\n",
    "\n",
    "        for idx in indexes:\n",
    "            image, caption, target = self.dataset[idx]\n",
    "            if image is not None and caption is not None:\n",
    "                batch_images.append(image)\n",
    "                batch_captions.append(caption)\n",
    "                batch_targets.append(target)\n",
    "\n",
    "        if batch_images and batch_captions:\n",
    "            batch_images = np.stack(batch_images, axis=0)\n",
    "            batch_captions = np.stack(batch_captions, axis=0)\n",
    "            batch_targets = np.stack(batch_targets, axis=0)\n",
    "        else:\n",
    "            batch_images = np.array([])\n",
    "            batch_captions = np.array([])\n",
    "            batch_targets = np.array([])\n",
    "\n",
    "        return [batch_images, batch_captions], batch_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexes) // self.batch_size\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5cc9dc-9dfa-40e1-bec1-b879674d9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = Dataset(train_data, input_size, tokenizer, max_pad, augment=True)\n",
    "val_dataset = Dataset(val_data, input_size, tokenizer, max_pad, augment=False)  # Usually, we don't apply augmentation to validation data\n",
    "test_dataset = Dataset(test_data, input_size, tokenizer, max_pad, augment=False)\n",
    "\n",
    "train_dataloader = Dataloader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = Dataloader(val_dataset, batch_size=batch_size)  # Create dataloader for validation data\n",
    "test_dataloader = Dataloader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Check the number of batches\n",
    "print(f\"Number of batches in train_dataloader: {len(train_dataloader)}\")\n",
    "print(f\"Number of batches in val_dataloader: {len(val_dataloader)}\")  # Print for validation dataloader\n",
    "print(f\"Number of batches in test_dataloader: {len(test_dataloader)}\")\n",
    "\n",
    "# Check a single batch from each dataloader\n",
    "train_batch = next(iter(train_dataloader))\n",
    "val_batch = next(iter(val_dataloader))  # Get a batch from val_dataloader\n",
    "test_batch = next(iter(test_dataloader))\n",
    "\n",
    "train_images, train_captions = train_batch[0]\n",
    "train_targets = train_batch[1]\n",
    "\n",
    "valid_images, valid_captions = val_batch[0]\n",
    "valid_targets = val_batch[1]\n",
    "\n",
    "test_images, test_captions = test_batch[0]\n",
    "test_targets = test_batch[1]\n",
    "\n",
    "print(f\"Shape of train images: {train_images.shape}\")\n",
    "print(f\"Shape of train captions: {train_captions.shape}\")\n",
    "print(f\"Shape of train targets: {train_targets.shape}\")\n",
    "\n",
    "print(f\"Shape of train images: {valid_images.shape}\")\n",
    "print(f\"Shape of train captions: {valid_captions.shape}\")\n",
    "print(f\"Shape of train targets: {valid_targets.shape}\")\n",
    "\n",
    "\n",
    "print(f\"Shape of test images: {test_images.shape}\")\n",
    "print(f\"Shape of test captions: {test_captions.shape}\")\n",
    "print(f\"Shape of test targets: {test_targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad1bdd4-ba76-4612-88e6-4dec215f9668",
   "metadata": {},
   "outputs": [],
   "source": [
    "chexnet_weights = \"brucechou1983_CheXNet_Keras_0.3.0_weights.h5\"\n",
    "\n",
    "def create_chexnet(chexnet_weights = chexnet_weights):\n",
    "    \"\"\"\n",
    "    chexnet_weights: weights value in .h5 format of chexnet\n",
    "    creates a chexnet model with preloaded weights present in chexnet_weights file\n",
    "    \"\"\"\n",
    "    model = tf.keras.applications.DenseNet121(include_top=False) #importing densenet the last layer will be a relu activation layer\n",
    "\n",
    "    #we need to load the weights so setting the architecture of the model as same as the one of tha chexnet\n",
    "    x = model.output #output from chexnet\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(14, activation=\"sigmoid\", name=\"chexnet_output\")(x) #here activation is sigmoid as seen in research paper\n",
    "\n",
    "    chexnet = tf.keras.Model(inputs = model.input,outputs = x)\n",
    "    chexnet.load_weights(chexnet_weights)\n",
    "    chexnet = tf.keras.Model(inputs = model.input,outputs = chexnet.layers[-2].output)  #we will be taking the penultimate layer (second last layer here it is global avgpooling)\n",
    "    return chexnet\n",
    "\n",
    "class Image_encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer will output image backbone features after passing it through chexnet\n",
    "    here chexnet will be not be trainable\n",
    "    \"\"\"\n",
    "    def __init__(self,name = \"image_encoder_block\"):\n",
    "        super().__init__()\n",
    "        self.chexnet = create_chexnet()\n",
    "        self.chexnet.trainable = False\n",
    "\n",
    "\n",
    "    def call(self,data):\n",
    "        op = self.chexnet(data)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34039c4-869d-469e-8cd5-a66e6232a491",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "vocab_size = len(vocab_list)\n",
    "with open('ChestIU X-Ray Dataset/glove_6B/glove.6B.300d.txt',encoding='utf-8') as f: #taking 300 dimesions\n",
    "    for line in f:\n",
    "        word = line.split()\n",
    "        glove[word[0]] = np.asarray(word[1:], dtype='float32')\n",
    "print(f\"Number of words loaded from GloVe: {len(glove)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "embedding_dim = 300\n",
    "# create a weight matrix for words in training docs for embedding purpose\n",
    "embedding_matrix = np.zeros((vocab_size+1, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = glove.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector[:embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5982fe74-8d8e-4004-8b51-44311e80eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "from tensorflow.keras.regularizers import l2\n",
    "image = Input(shape = (input_size + (3,))) #shape = 224,224,3\n",
    "caption = Input(shape = (max_pad,))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "img_encoder = Image_encoder()\n",
    "bk_feat = img_encoder(image)\n",
    "image_dense = Dense(dense_dim,\n",
    "                    activation = 'relu',\n",
    "                    name = 'Image_dense',\n",
    "                    use_bias='False',\n",
    "                    kernel_regularizer=l2(0.01))                    \n",
    "image_bkbone = image_dense(bk_feat)\n",
    "image_dense_op = tf.keras.backend.expand_dims(image_bkbone,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# embedding = Embedding(input_dim  = vocab_size+1,\n",
    "#                               output_dim = embedding_dim,\n",
    "#                               input_length = max_pad,\n",
    "#                               mask_zero = True,\n",
    "#                               weights = [embedding_matrix],\n",
    "#                               name = 'embedding')\n",
    "\n",
    "embedding = Embedding(input_dim=vocab_size+1,\n",
    "                      output_dim=embedding_dim,\n",
    "                      input_length=max_pad,\n",
    "                      mask_zero=True,\n",
    "                      trainable=True,  # Ensure the embedding layer is trainable\n",
    "                      name='embedding')\n",
    "\n",
    "\n",
    "embed_op = embedding(caption)\n",
    "\n",
    "lstm_layer = LSTM(units = lstm_units,\n",
    "                  return_sequences= True,\n",
    "                  return_state = True)\n",
    "\n",
    "lstm_op,lstm_h,lstm_c = lstm_layer(embed_op,initial_state = [image_bkbone,image_bkbone]) #op_shape = batch_size*input_length*lstm_units\n",
    "\n",
    "lstm_op = BatchNormalization()(lstm_op)\n",
    "add = Add()([image_dense_op,lstm_op])\n",
    "\n",
    "op_dense = Dense(vocab_size+1,\n",
    "                 activation = 'softmax',\n",
    "                 name = 'output_dense')\n",
    "\n",
    "output = op_dense(add)\n",
    "\n",
    "model = tf.keras.Model(inputs = [image,caption], outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532bd34c-94f5-4bc0-b3cf-85b73e3fc1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb2e1d6-782b-4982-ac7e-7b18ed4e44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy() \n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    #getting mask value to not consider those words which are not present in the true caption\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "\n",
    "    #calculating the loss\n",
    "    loss_ = loss_func(y_true, y_pred)\n",
    "    \n",
    "    #converting mask dtype to loss_ dtype\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    \n",
    "    #applying the mask to loss\n",
    "    loss_ = loss_*mask\n",
    "    \n",
    "    #returning mean over all the values\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "# # @tf.keras.utils.register_keras_serializable()\n",
    "def custom_accuracy(y_true, y_pred):\n",
    "    # Identify non-padding tokens\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
    "    \n",
    "    # Predicted class (highest probability)\n",
    "    y_pred_classes = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n",
    "    \n",
    "    # Ensure y_true is also of type int32 for the comparison\n",
    "    y_true_int = tf.cast(y_true, tf.int32)\n",
    "    \n",
    "    # Boolean tensor of correct predictions, ignoring padding\n",
    "    correct_predictions = tf.cast(tf.equal(y_true_int, y_pred_classes), dtype=tf.float32) * mask\n",
    "    \n",
    "    # Sum of correct predictions / Sum of non-padding tokens\n",
    "    accuracy = tf.reduce_sum(correct_predictions) / (tf.reduce_sum(mask) + tf.keras.backend.epsilon())\n",
    "\n",
    "    # # Debugging print statements\n",
    "    # tf.print(\"Mask:\", mask)\n",
    "    # tf.print(\"Correct predictions:\", correct_predictions)\n",
    "    # tf.print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "lr = 10**-3\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = lr)   \n",
    "model.compile(optimizer=optimizer,loss=loss_func, metrics= [custom_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7122e0-f445-44ba-9be8-b4e990bab5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model_save = \"saved_models/cnn_lstm.keras\"\n",
    "my_callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(patience = 5,\n",
    "                                     verbose = 2),\n",
    "    tf.keras.callbacks.ModelCheckpoint(filepath=model_save,\n",
    "                                       save_best_only = True,\n",
    "                                       save_weights_only = False,\n",
    "                                       verbose = 2),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=2, min_lr=10**-7, verbose = 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e63ae-0ac7-47b6-b067-5244baa5fc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataloader,\n",
    "          validation_data = test_dataloader,\n",
    "          epochs = 50,\n",
    "          callbacks = my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa889db8-84ab-435a-8230-28d8c09bcc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting loss and other metrics from the history object\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Update these lines with the correct keys\n",
    "acc = history.history['custom_accuracy']  # Assuming 'acc' is the correct key for accuracy\n",
    "val_acc = history.history['val_custom_accuracy']  # And 'val_acc' for validation accuracy\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Plotting training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'b-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, acc, 'b-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65996371-5215-4686-a477-7638faa6cf36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455d443-98f2-4dec-9548-88be0fec1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the test dataloader\n",
    "for batch in test_dataloader:\n",
    "    # Unpack the batch\n",
    "    (img_batch, caption_batch), _ = batch  # No need for targets during inference\n",
    "    sample_img = img_batch[0].numpy()  # Convert the first image in the batch from tensor to numpy array\n",
    "    \n",
    "    print(\"Image shape:\", sample_img.shape)\n",
    "    plt.imshow(sample_img)\n",
    "    plt.title(\"Sample Image\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Expand dimensions to fit the model input\n",
    "    image = np.expand_dims(sample_img, axis=0)  # Adjust if your model expects a different input shape\n",
    "    print(\"Image shape after reshaping:\", image.shape)\n",
    "    \n",
    "    # Process the image through your model\n",
    "    cnn_img = caption_model.cnn_model(image)\n",
    "    encoded_img = caption_model.encoder(cnn_img, training=False)\n",
    "    \n",
    "    vocab = vectorization.get_vocabulary()\n",
    "    index_lookup = dict(zip(range(len(vocab)), vocab))\n",
    "    max_decoded_sentence_length = SEQ_LENGTH - 1\n",
    "    \n",
    "    # Since we are not using 'caption' from the dataloader directly, we skip processing it here\n",
    "    # Begin the decoding process\n",
    "    decoded_caption = \"<start>\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
    "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
    "        predictions = caption_model.decoder(\n",
    "            tokenized_caption, encoded_img, training=False, mask=mask\n",
    "        )\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = index_lookup[sampled_token_index]\n",
    "        if sampled_token == \"<end>\":\n",
    "            break\n",
    "        decoded_caption += \" \" + sampled_token\n",
    "    \n",
    "    decoded_caption = decoded_caption.replace(\"<start> \", \"\").replace(\" <end>\", \"\").strip()\n",
    "    print(\"Predicted Caption:\", decoded_caption)\n",
    "    \n",
    "    # Break after displaying the first image and its caption\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48a88bf-5e6a-4ad7-843b-0bbde007b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in test_dataloader:\n",
    "    (img_batch, caption_batch), _ = batch\n",
    "    plt_img = img_batch[0]\n",
    "    sample_img = img_batch[0:1]  # Ensure this is a batch of size 1 for model prediction\n",
    "    sample_cap = caption_batch[0:1]  # Ensure this is a batch of size 1 for model prediction\n",
    "    \n",
    "    # Display the sample image\n",
    "    plt.imshow(plt_img)  # Convert to numpy array if needed\n",
    "    plt.title(\"Sample Image\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Predict caption\n",
    "    pred = model.predict([sample_img, sample_cap])\n",
    "    predicted_token_indices = np.argmax(pred, axis=-1)[0]  # Take the first sequence (batch size = 1) and get the argmax for each time step\n",
    "\n",
    "    predicted_words = []\n",
    "    for idx in predicted_token_indices:\n",
    "        word = tokenizer.index_word.get(idx, '')\n",
    "        if word == 'end':  # Ensure this matches the special token used during training\n",
    "            break  # Stop adding words once the '<end>' token is encountered\n",
    "        if word and word not in ['<start>', '<pad>']:  # Optionally skip '<start>' and '<pad>' tokens\n",
    "            predicted_words.append(word)\n",
    "\n",
    "    predicted_caption = ' '.join(predicted_words)\n",
    "    print(\"Predicted Caption:\", predicted_caption)\n",
    "\n",
    "    # Find the actual caption\n",
    "    actual_tokens = [int(token) for token in sample_cap[0] if token != 0]  # Filter out padding tokens (assuming 0 is the padding token)\n",
    "    actual_words = [tokenizer.index_word.get(token, '') for token in actual_tokens]\n",
    "    actual_caption = ' '.join(word for word in actual_words if word not in ['start', 'end', '<pad>'])  # Optionally skip special tokens\n",
    "    print(\"Actual Caption:\", actual_caption)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35363c2f-2428-4b95-9e4a-5e863243b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predicted_captions = []\n",
    "actual_captions = []\n",
    "\n",
    "for batch in tqdm(test_dataloader, desc=\"Predicting for Test Dataset...\"):\n",
    "    (img_batch, caption_batch), _ = batch\n",
    "    \n",
    "    # Predict caption\n",
    "    pred = model.predict([img_batch, caption_batch])\n",
    "    predicted_token_indices_batch = np.argmax(pred, axis=-1)  # Process the whole batch\n",
    "    \n",
    "    for idx, predicted_token_indices in enumerate(predicted_token_indices_batch):\n",
    "        predicted_words = []\n",
    "        for token_idx in predicted_token_indices:\n",
    "            word = tokenizer.index_word.get(token_idx, '')\n",
    "            if word == 'end':  # Ensure this matches the special token used during training\n",
    "                break  # Stop adding words once the '<end>' token is encountered\n",
    "            if word and word not in ['start', '<OOV>']:  # Optionally skip '<start>' and '<OOV>' tokens\n",
    "                predicted_words.append(word)\n",
    "        predicted_caption = ' '.join(predicted_words)\n",
    "        predicted_captions.append(predicted_caption)\n",
    "        \n",
    "        # Find the actual caption\n",
    "        actual_tokens = [int(token) for token in caption_batch[idx] if token != 0]  # Filter out padding tokens (assuming 0 is the padding token)\n",
    "        actual_words = [tokenizer.index_word.get(token, '') for token in actual_tokens]\n",
    "        actual_caption = ' '.join(word for word in actual_words if word not in ['start', 'end', '<pad>'])  # Optionally skip special tokens\n",
    "        actual_captions.append(actual_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b6e44d-7eff-4768-8ed3-d7f1c240442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu(reference,prediction):\n",
    "    \"\"\"\n",
    "    Given a reference and prediction string, outputs the 1-gram,2-gram,3-gram and 4-gram bleu scores\n",
    "    \"\"\"\n",
    "    reference = [reference.split()] #should be in an array (cos of multiple references can be there here only 1)\n",
    "    prediction = prediction.split()\n",
    "    bleu1 = sentence_bleu(reference,prediction,weights = (1,0,0,0))\n",
    "    bleu2 = sentence_bleu(reference,prediction,weights = (0.5,0.5,0,0))\n",
    "    bleu3 = sentence_bleu(reference,prediction,weights = (0.33,0.33,0.33,0))\n",
    "    bleu4 = sentence_bleu(reference,prediction,weights = (0.25,0.25,0.25,0.25))\n",
    "\n",
    "    return bleu1,bleu2,bleu3,bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a3e26f-8262-41de-9db9-5d2757199132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def mean_bleu(pred_ls, act_ls, **kwargs):\n",
    "\n",
    "    bleu1, bleu2, bleu3, bleu4 = [], [], [], []\n",
    "\n",
    "    for k in range (len(pred_ls)):\n",
    "        # Tokenize the true and predicted captions\n",
    "        true_tokens = act_ls[k]\n",
    "        #print (\"True: \",true_tokens)\n",
    "        predict_tokens = pred_ls[k]\n",
    "        #print (\"Predicted: \",predict_tokens)\n",
    "        \n",
    "        # Calculate BLEU scores\n",
    "        bleu1.append(sentence_bleu([true_tokens], predict_tokens, weights=(1, 0, 0, 0)))\n",
    "        bleu2.append(sentence_bleu([true_tokens], predict_tokens, weights=(0.5, 0.5, 0, 0)))\n",
    "        bleu3.append(sentence_bleu([true_tokens], predict_tokens, weights=(0.33, 0.33, 0.33, 0)))\n",
    "        bleu4.append(sentence_bleu([true_tokens], predict_tokens, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "    return np.array(bleu1).mean(), np.array(bleu2).mean(), np.array(bleu3).mean(), np.array(bleu4).mean()\n",
    "bleu1,bleu2,bleu3,bleu4 = mean_bleu(predicted_captions,actual_captions)\n",
    "print (\"Bleu Score 1-gram: \",bleu1)\n",
    "print (\"Bleu Score 2-gram: \",bleu2)\n",
    "print (\"Bleu Score 3-gram: \",bleu3)\n",
    "print (\"Bleu Score 4-gram: \",bleu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915a4e8-daea-4491-8b32-b9f24ba93026",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61cb0b-7b87-4485-a243-aba376153876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "filtered_predicted_captions = [pred for i, pred in enumerate(predicted_captions) if actual_captions[i].strip()]\n",
    "filtered_actual_captions = [act for act in actual_captions if act.strip()]\n",
    "\n",
    "\n",
    "rouge_scores = calculate_rouge_scores(filtered_predicted_captions, filtered_actual_captions)\n",
    "print(rouge_scores)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rouge_scores(predictions, references):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predictions, references, avg=True)\n",
    "    return scores\n",
    "\n",
    "rouge_scores = calculate_rouge_scores(filtered_predicted_captions, filtered_actual_captions)\n",
    "print(rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6544c-f137-4541-8c5f-4e56e5a6601d",
   "metadata": {},
   "source": [
    "ROUGE-1: Measures the overlap of unigrams (single words) between the predicted and actual captions.\n",
    "\n",
    "Recall (r): 18.75% of the words in the actual captions are also found in the predicted captions.\n",
    "Precision (p): 26.29% of the words in the predicted captions are correct (i.e., they appear in the actual captions).\n",
    "F1-score (f): The harmonic mean of precision and recall is 20.92%, indicating the overall unigram overlap efficiency.\n",
    "ROUGE-2: Measures the overlap of bigrams (pairs of consecutive words) between the predicted and actual captions.\n",
    "\n",
    "Recall (r): 4.30% of the bigrams in the actual captions are also found in the predicted captions.\n",
    "Precision (p): 5.22% of the bigrams in the predicted captions are correct.\n",
    "F1-score (f): The harmonic mean of precision and recall is 4.50%, indicating the model's efficiency in capturing two-word phrases.\n",
    "ROUGE-L: Focuses on the longest common subsequence and can capture longer-term dependencies than ROUGE-1 or ROUGE-2, giving a sense of the overall structure captured by the predictions.\n",
    "\n",
    "Recall (r): 17.88% of the longest sequences in the actual captions are also found in the predicted captions.\n",
    "Precision (p): 24.97% of the sequences in the predicted captions are correct.\n",
    "F1-score (f): The harmonic mean of precision and recall is 19.93%, indicating the model's effectiveness in capturing longer sequence patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89e2a43-688a-4df9-8815-c222745f493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, caption in enumerate(actual_captions):\n",
    "    if not caption.strip():\n",
    "        print(f\"Empty actual caption at index {i}\")\n",
    "for i, caption in enumerate(predicted_captions):\n",
    "    if not caption.strip():\n",
    "        print(f\"Empty predicted caption at index {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042df007-ce6b-4af6-bbe3-ae74c7ad5ad9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda4b83e-dd09-40ae-a46c-23b10b8a2105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
