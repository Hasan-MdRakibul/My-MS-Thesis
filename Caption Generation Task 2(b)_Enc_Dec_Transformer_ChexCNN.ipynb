{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1235bd5a-71a0-4afc-a197-4fa073b19611",
   "metadata": {},
   "source": [
    "## Transfomer-based Encoder Decoder with Pretrained Dense121+CheXNet Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69756868-91e5-41bd-9914-ab30e99d2321",
   "metadata": {},
   "source": [
    "#### 1. Importing Libraries and Configuring the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653038e1-0dab-4fa7-afa3-e8ee412087c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import Counter\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from keras.layers import TextVectorization\n",
    "import keras\n",
    "from keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Reshape, Conv2D, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8b4d71-ae32-49b3-9b83-56d080586e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_gpus():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(f\"{len(gpus)} Physical GPU(s), {len(logical_gpus)} Logical GPU(s) configured.\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError in configuring GPUs: {e}\")\n",
    "    else:\n",
    "        print(\"No GPU is available.\")\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "def check_jupyter_notebook():\n",
    "    try:\n",
    "        cfg = get_ipython().config \n",
    "        print(\"Jupyter Notebook environment detected. Configuring...\")\n",
    "        cfg.NotebookApp.iopub_msg_rate_limit = 10000.0\n",
    "        cfg.NotebookApp.rate_limit_window = 5.0\n",
    "    except NameError:\n",
    "        print(\"Not running in a Jupyter Notebook environment.\")\n",
    "\n",
    "# Clear TensorFlow session and suppress warnings\n",
    "tf.keras.backend.clear_session()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GPU configuration and seed setting\n",
    "configure_gpus()\n",
    "seed_everything()\n",
    "\n",
    "# Check if running in Jupyter Notebook and configure\n",
    "check_jupyter_notebook()\n",
    "\n",
    "# Initialize TensorFlow distributed strategy\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d11b8a-0d60-4d00-b142-eb4d0f160f77",
   "metadata": {},
   "source": [
    "#### 2. Train, Test and Validation Data Preparation (Image Paths, Pre-processed Captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9527e5b1-9560-4c45-965f-f56ee5ef247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class CaptionDataProcessor:\n",
    "        def __init__(self, train_csv, valid_csv, train_images_path, valid_images_path):\n",
    "            self.train_csv = train_csv\n",
    "            self.valid_csv = valid_csv\n",
    "            self.train_images_path = train_images_path\n",
    "            self.valid_images_path = valid_images_path\n",
    "            self.image_caption_pairs = {}\n",
    "            self.caption_ls = []\n",
    "            self.vocab_list = set()\n",
    "            self.cap_len = []\n",
    "\n",
    "        def load_data(self):\n",
    "            df_train = pd.read_csv(self.train_csv, delimiter='\\t')\n",
    "            df_valid = pd.read_csv(self.valid_csv, delimiter='\\t')\n",
    "            self.process_data(df_train, self.train_images_path, \"Training Image and Caption Data\")\n",
    "            self.process_data(df_valid, self.valid_images_path, \"Validation Image and Caption Data\")\n",
    "\n",
    "        def process_caption(self, text):\n",
    "            # Lowercase the caption label\n",
    "            caption_labels_lower = text.lower()\n",
    "            caption_labels_lower = caption_labels_lower.strip()\n",
    "\n",
    "            # Replace hyphens with spaces\n",
    "            caption_labels_clean = re.sub(r'-', ' ', caption_labels_lower)\n",
    "\n",
    "            # Remove special characters using regular expressions\n",
    "            caption_labels_clean = re.sub(r'[^a-zA-Z0-9\\s]', '', caption_labels_clean)\n",
    "\n",
    "            # Tokenize the caption label into words\n",
    "            words = caption_labels_clean.split()\n",
    "\n",
    "            # Remove numbers, words containing any numeric values\n",
    "            processed_words = [word for word in words if not word.isdigit() and not any(char.isdigit() for char in word)]\n",
    "\n",
    "            # Join the processed words back into a sentence\n",
    "            caption_labels_processed = ' '.join(processed_words)\n",
    "            caption_labels_processed = '<START> '+caption_labels_processed+ ' <END>'\n",
    "            return caption_labels_processed\n",
    "\n",
    "        def process_data(self, df, image_path_prefix, desc):\n",
    "            for index, row in tqdm(df.iterrows(), total=df.shape[0], desc=desc):\n",
    "                img_id, img_caption = row['ID'], row['caption']\n",
    "                img_path = os.path.join(image_path_prefix, str(img_id) + '.jpg')\n",
    "                caption = self.process_caption(img_caption)\n",
    "\n",
    "                if os.path.exists(img_path):\n",
    "                    if img_path not in self.image_caption_pairs:\n",
    "                        self.image_caption_pairs[img_path] = []\n",
    "\n",
    "                    self.image_caption_pairs[img_path].append(caption)\n",
    "                    self.caption_ls.append(caption)\n",
    "                    self.vocab_list.update(caption.split(' '))\n",
    "                    words = caption.split(' ')\n",
    "                    self.cap_len.append(len(words))\n",
    "                else:\n",
    "                    print(f\"File not found: {img_path}\")\n",
    "\n",
    "\n",
    "\n",
    "        def save_datasets(self, save_dir):\n",
    "            \"\"\"Saves the main dataset and the train, validation, and test splits to files.\"\"\"\n",
    "            try:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                self._save_dataset(os.path.join(save_dir, 'main_dataset.pkl'), self.image_caption_pairs)\n",
    "                self._save_dataset(os.path.join(save_dir, 'train.pkl'), self.training_data)\n",
    "                self._save_dataset(os.path.join(save_dir, 'valid.pkl'), self.validation_data)\n",
    "                self._save_dataset(os.path.join(save_dir, 'test.pkl'), self.test_data)\n",
    "                print(\"Datasets saved successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving datasets: {e}\")\n",
    "\n",
    "        def _save_dataset(self, file_path, dataset):\n",
    "            \"\"\"Helper method to save a single dataset to a file.\"\"\"\n",
    "            with open(file_path, 'wb') as file:\n",
    "                pickle.dump(dataset, file)\n",
    "        \n",
    "        \n",
    "        \n",
    "        def train_val_test_split(self, train_size=0.8, val_size=0.15, shuffle=True):\n",
    "            all_images = list(self.image_caption_pairs.keys())\n",
    "            if shuffle:\n",
    "                np.random.shuffle(all_images)\n",
    "\n",
    "            total_size = len(self.image_caption_pairs)\n",
    "            train_end = int(total_size * train_size)\n",
    "            val_end = train_end + int(total_size * val_size)\n",
    "\n",
    "            # Properly set the class attributes for the splits\n",
    "            self.training_data = {img_name: self.image_caption_pairs[img_name] for img_name in all_images[:train_end]}\n",
    "            self.validation_data = {img_name: self.image_caption_pairs[img_name] for img_name in all_images[train_end:val_end]}\n",
    "            self.test_data = {img_name: self.image_caption_pairs[img_name] for img_name in all_images[val_end:]}\n",
    "\n",
    "            # Return the splits for external use as well\n",
    "            return self.training_data, self.validation_data, self.test_data\n",
    "\n",
    "    processor = CaptionDataProcessor(\n",
    "        'ImageCLEFmedical_Caption_2023_caption_prediction_train_labels.csv',\n",
    "        'ImageCLEFmedical_Caption_2023_caption_prediction_valid_labels.csv',\n",
    "        'ImageCLEFmedical_Caption_2023_train_images',\n",
    "        'ImageCLEFmedical_Caption_2023_valid_images'\n",
    "    )\n",
    "    processor.load_data()\n",
    "    train_data, valid_data, test_data = processor.train_val_test_split()\n",
    "    processor.save_datasets('pickles_clef23')\n",
    "\n",
    "    print(\"Number of records in the main dataset: \", len(processor.image_caption_pairs))\n",
    "    print(\"Number of training samples: \", len(train_data))\n",
    "    print(\"Number of validation samples: \", len(valid_data))\n",
    "    print(\"Number of test samples: \", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e83e3a-4e64-45fa-bb7a-cbe51f62cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_data(data, title, sample_count=3):\n",
    "    \"\"\"Displays a sample of image paths and their corresponding captions.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The dataset containing image paths as keys and captions as values.\n",
    "        title (str): Title to describe the dataset being displayed.\n",
    "        sample_count (int): Number of samples to display.\n",
    "    \"\"\"\n",
    "    print(f\"--- {title} ---\")\n",
    "    image_paths = list(data.keys())\n",
    "    captions = list(data.values())\n",
    "\n",
    "    for i in range(sample_count):\n",
    "        print(f\"\\nImage Path {i + 1}: {image_paths[i]}\")\n",
    "        print(f\"Pre-processed Caption {i + 1}: {captions[i]}\")\n",
    "    \n",
    "    print(\"-------------------------------------------------------------------------\\n\")\n",
    "\n",
    "# Display sample data from train, validation, and test datasets\n",
    "display_sample_data(train_data, \"Sample Train Data\")\n",
    "display_sample_data(valid_data, \"Sample Validation Data\")\n",
    "display_sample_data(test_data, \"Sample Test Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14f6b7d-208e-48fc-a21c-75c412c4fc33",
   "metadata": {},
   "source": [
    "#### 3. Max Length Determination with Other Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02d77f2-304b-4ad0-ba54-5c93a7ff25a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Finding the Max Length\n",
    "\n",
    "data = pd.read_pickle('pickles_clef23/main_dataset.pkl')\n",
    "\n",
    "# Calculate frequencies\n",
    "frequency = Counter(processor.cap_len)\n",
    "total_items = sum(frequency.values())\n",
    "sorted_frequency = sorted(frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "\n",
    "# print(\"Frequency of numbers in descending order:\")\n",
    "# for num, freq in sorted_frequency:\n",
    "#     print(f\"Number {num}: Frequency {freq}\")\n",
    "\n",
    "\n",
    "# Calculate cumulative frequency and find the 90% point\n",
    "cumulative = 0\n",
    "ninety_percent_mark = total_items * 0.99\n",
    "for num, freq in sorted_frequency:\n",
    "    cumulative += freq\n",
    "    if cumulative >= ninety_percent_mark:\n",
    "        print(f\"99% images have maximum length of caption is: {num}\")\n",
    "        print (\"This should be the max length\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95dd344-f247-4a6e-be59-5661d59746ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (224, 224)\n",
    "VOCAB_SIZE = len(processor.vocab_list)\n",
    "SEQ_LENGTH = num\n",
    "\n",
    "# Dimension for the image embeddings and token embeddings\n",
    "EMBED_DIM = 512\n",
    "\n",
    "# Per-layer units in the feed-forward network\n",
    "FF_DIM = 512\n",
    "\n",
    "# Other training parameters\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f86792-6465-4eb3-95bb-711b20ed69aa",
   "metadata": {},
   "source": [
    "#### 4. Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6787f74-699e-4f51-8b4a-3221d085312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def custom_standardization(input_string):\n",
    "        lowercase = tf.strings.lower(input_string)\n",
    "        return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "    strip_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "    strip_chars = strip_chars.replace(\"<\", \"\")\n",
    "    strip_chars = strip_chars.replace(\">\", \"\")\n",
    "\n",
    "\n",
    "    vectorization = TextVectorization(\n",
    "        max_tokens=VOCAB_SIZE,\n",
    "        output_mode=\"int\",\n",
    "        output_sequence_length=SEQ_LENGTH,\n",
    "        standardize=custom_standardization,\n",
    "    )\n",
    "    vectorization.adapt(processor.caption_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6217639d-5ea5-4eb7-becb-b8f43cd5ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example captions\n",
    "example_captions = processor.caption_ls[:3]\n",
    "\n",
    "# Vectorize the example captions\n",
    "vectorized_captions = vectorization(example_captions)\n",
    "\n",
    "# Print the original and vectorized captions\n",
    "for original, vectorized in zip(example_captions, vectorized_captions.numpy()):\n",
    "    print(\"Original:\", original)\n",
    "    print(\"Vectorized:\", vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53527be-9402-4167-8952-f3f0b3c4e12c",
   "metadata": {},
   "source": [
    "#### 5. Data Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df264dc8-e63d-4a0a-91a7-3961231789e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class DatasetBuilder:\n",
    "        def __init__(self, vectorization_layer, batch_size, img_size, autotune=tf.data.AUTOTUNE):\n",
    "            self.vectorization_layer = vectorization_layer\n",
    "            self.batch_size = batch_size\n",
    "            self.img_size = img_size\n",
    "            self.autotune = autotune\n",
    "\n",
    "        def decode_and_resize(self, img_path):\n",
    "            img = tf.io.read_file(img_path)\n",
    "            img = tf.image.decode_jpeg(img, channels=3)\n",
    "            img = tf.image.resize(img, self.img_size)\n",
    "            img = tf.cast(img, tf.float32) / 255.0\n",
    "            return img\n",
    "\n",
    "        def process_input(self, img_path, caption):\n",
    "            img = self.decode_and_resize(img_path)\n",
    "            caption = tf.expand_dims(caption, 0)\n",
    "            caption = self.vectorization_layer(caption)\n",
    "            return img, tf.squeeze(caption, 0)\n",
    "\n",
    "        def make_dataset(self, image_paths, captions):\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((image_paths, captions))\n",
    "            dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "            dataset = dataset.map(self.process_input, num_parallel_calls=self.autotune)\n",
    "            dataset = dataset.batch(self.batch_size).prefetch(self.autotune)\n",
    "            return dataset\n",
    "\n",
    "\n",
    "    dataset_builder = DatasetBuilder(vectorization,BATCH_SIZE,IMAGE_SIZE)\n",
    "\n",
    "    # Assuming 'train_data', 'valid_data', and 'test_data' are available\n",
    "    train_dataset = dataset_builder.make_dataset(list(train_data.keys()), list(train_data.values()))\n",
    "    valid_dataset = dataset_builder.make_dataset(list(valid_data.keys()), list(valid_data.values()))\n",
    "    test_dataset = dataset_builder.make_dataset(list(test_data.keys()), list(test_data.values()))\n",
    "\n",
    "    # To print example data\n",
    "    for img, caption in train_dataset.take(1):\n",
    "        print(\"Image shape:\", img.numpy().shape)\n",
    "\n",
    "        # Display the first image in the batch\n",
    "        plt.imshow(img.numpy()[0])\n",
    "        plt.title(\"Sample Image\")\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Caption shape:\", caption.numpy().shape)\n",
    "        print(\"Caption:\", caption.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad32fa-1d02-450c-ab37-cd852a164ef8",
   "metadata": {},
   "source": [
    "#### 6. Dense121 with ChexNet Weights for Extracting Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a313e-a076-4678-a949-bac9d47aae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "chexnet_weights = \"brucechou1983_CheXNet_Keras_0.3.0_weights.h5\"\n",
    "\n",
    "with strategy.scope():\n",
    "    def get_cnn_model():\n",
    "        base_model = DenseNet121(include_top=False, input_shape=(*IMAGE_SIZE, 3))\n",
    "        x = base_model.output\n",
    "        x = GlobalAveragePooling2D()(x)\n",
    "        x = Dense(14, activation=\"sigmoid\", name=\"chexnet_output\")(x)\n",
    "        chexnet = tf.keras.Model(inputs = base_model.input,outputs = x)\n",
    "        chexnet.load_weights(chexnet_weights)\n",
    "        x = chexnet.get_layer('relu').output\n",
    "        x = Conv2D(2048, (1, 1), padding='same', activation='relu')(x)\n",
    "        x = Reshape((49, 2048))(x)\n",
    "        cnn_model = tf.keras.Model(inputs=base_model.input, outputs=x)\n",
    "        return cnn_model\n",
    "#cnn_model.summary()\n",
    "\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False\n",
    "# for layer in cnn_model.layers:\n",
    "#     print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6908b2-1587-4e5b-93b7-bf516f9a4d83",
   "metadata": {},
   "source": [
    "#### 7. Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7c53f6-163c-4ea4-a78a-97ae52d21ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class TransformerEncoderBlock(layers.Layer):\n",
    "        def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.embed_dim = embed_dim\n",
    "            self.dense_dim = dense_dim\n",
    "            self.num_heads = num_heads\n",
    "            self.attention_1 = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=embed_dim, dropout=0.0\n",
    "            )\n",
    "            self.layernorm_1 = layers.LayerNormalization()\n",
    "            self.layernorm_2 = layers.LayerNormalization()\n",
    "            self.dense_1 = layers.Dense(embed_dim, activation=\"relu\")\n",
    "\n",
    "        def call(self, inputs, training, mask=None):\n",
    "            #print(\"Encoder input shape:\", tf.shape(inputs))\n",
    "            inputs = self.layernorm_1(inputs)\n",
    "            inputs = self.dense_1(inputs)\n",
    "\n",
    "            attention_output_1 = self.attention_1(\n",
    "                query=inputs,\n",
    "                value=inputs,\n",
    "                key=inputs,\n",
    "                attention_mask=None,\n",
    "                training=training,\n",
    "            )\n",
    "            out_1 = self.layernorm_2(inputs + attention_output_1)\n",
    "            #print(\"Encoder output shape:\", tf.shape(out_1))\n",
    "            return out_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc29f8-a505-4534-85d6-518ef1fa7ded",
   "metadata": {},
   "source": [
    "#### 8. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c07da0-b880-4601-b288-35c8d62b84f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class PositionalEmbedding(layers.Layer):\n",
    "        def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.token_embeddings = layers.Embedding(\n",
    "                input_dim=vocab_size, output_dim=embed_dim\n",
    "            )\n",
    "            self.position_embeddings = layers.Embedding(\n",
    "                input_dim=sequence_length, output_dim=embed_dim\n",
    "            )\n",
    "            self.sequence_length = sequence_length\n",
    "            self.vocab_size = vocab_size\n",
    "            self.embed_dim = embed_dim\n",
    "            self.embed_scale = tf.math.sqrt(tf.cast(embed_dim, tf.float32))\n",
    "\n",
    "        def call(self, inputs):\n",
    "            #print(\"Positional Embedding input shape:\", tf.shape(inputs))\n",
    "            length = tf.shape(inputs)[-1]\n",
    "            positions = tf.range(start=0, limit=length, delta=1)\n",
    "            embedded_tokens = self.token_embeddings(inputs)\n",
    "            embedded_tokens = embedded_tokens * self.embed_scale\n",
    "            embedded_positions = self.position_embeddings(positions)\n",
    "            final_embeddings = embedded_tokens + embedded_positions\n",
    "\n",
    "            # Checking the shape of the output tensor\n",
    "            #print(\"Positional Embedding Output shape:\", tf.shape(final_embeddings))\n",
    "\n",
    "            return final_embeddings\n",
    "\n",
    "        def compute_mask(self, inputs, mask=None):\n",
    "            return tf.math.not_equal(inputs, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d112c7-b315-46d4-84cd-5c7b5d1cb040",
   "metadata": {},
   "source": [
    "#### 9. Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd120bee-8c7a-4043-95fe-a74d49e99ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class TransformerDecoderBlock(layers.Layer):\n",
    "        def __init__(self, embed_dim, ff_dim, num_heads, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.embed_dim = embed_dim\n",
    "            self.ff_dim = ff_dim\n",
    "            self.num_heads = num_heads\n",
    "            self.attention_1 = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "            )\n",
    "            self.attention_2 = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
    "            )\n",
    "            self.ffn_layer_1 = layers.Dense(ff_dim, activation=\"relu\")\n",
    "            self.ffn_layer_2 = layers.Dense(embed_dim)\n",
    "\n",
    "            self.layernorm_1 = layers.LayerNormalization()\n",
    "            self.layernorm_2 = layers.LayerNormalization()\n",
    "            self.layernorm_3 = layers.LayerNormalization()\n",
    "\n",
    "            self.embedding = PositionalEmbedding(\n",
    "                embed_dim=EMBED_DIM,\n",
    "                sequence_length=SEQ_LENGTH,\n",
    "                vocab_size=VOCAB_SIZE,\n",
    "            )\n",
    "            self.out = layers.Dense(VOCAB_SIZE, activation=\"softmax\")\n",
    "\n",
    "            self.dropout_1 = layers.Dropout(0.3)\n",
    "            self.dropout_2 = layers.Dropout(0.5)\n",
    "            self.supports_masking = True\n",
    "\n",
    "        def call(self, inputs, encoder_outputs, training, mask=None):\n",
    "            # Print input shapes\n",
    "            #print(\"Decoder input shape:\", tf.shape(inputs))\n",
    "            #print(\"Encoder output shape:\", tf.shape(encoder_outputs))\n",
    "            inputs = self.embedding(inputs)\n",
    "            #print(\"Shape after embedding in Decoder Block:\", tf.shape(inputs))\n",
    "            causal_mask = self.get_causal_attention_mask(inputs)\n",
    "\n",
    "            if mask is not None:\n",
    "                padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
    "                combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
    "                combined_mask = tf.minimum(combined_mask, causal_mask)\n",
    "\n",
    "            attention_output_1 = self.attention_1(\n",
    "                query=inputs,\n",
    "                value=inputs,\n",
    "                key=inputs,\n",
    "                attention_mask=combined_mask,\n",
    "                training=training,\n",
    "            )\n",
    "            out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "            #print(\"Shape after first attention:\", tf.shape(out_1))\n",
    "\n",
    "            attention_output_2 = self.attention_2(\n",
    "                query=out_1,\n",
    "                value=encoder_outputs,\n",
    "                key=encoder_outputs,\n",
    "                attention_mask=padding_mask,\n",
    "                training=training,\n",
    "            )\n",
    "            out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "            #print(\"Shape after second attention:\", tf.shape(out_2))\n",
    "            ffn_out = self.ffn_layer_1(out_2)\n",
    "            ffn_out = self.dropout_1(ffn_out, training=training)\n",
    "            ffn_out = self.ffn_layer_2(ffn_out)\n",
    "\n",
    "            ffn_out = self.layernorm_3(ffn_out + out_2, training=training)\n",
    "            ffn_out = self.dropout_2(ffn_out, training=training)\n",
    "            preds = self.out(ffn_out)\n",
    "            #print(\"Decoder final output shape:\", tf.shape(preds))\n",
    "            return preds\n",
    "\n",
    "        def get_causal_attention_mask(self, inputs):\n",
    "            input_shape = tf.shape(inputs)\n",
    "            batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "            i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "            j = tf.range(sequence_length)\n",
    "            mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "            mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "            mult = tf.concat(\n",
    "                [\n",
    "                    tf.expand_dims(batch_size, -1),\n",
    "                    tf.constant([1, 1], dtype=tf.int32),\n",
    "                ],\n",
    "                axis=0,\n",
    "            )\n",
    "            return tf.tile(mask, mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88508f3c-2d09-4f53-8b79-601f1c0ae2fa",
   "metadata": {},
   "source": [
    "#### 10. Image Captioning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124a869-c7fa-4677-b7a5-a69064fa6be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class ImageCaptioningModel(keras.Model):\n",
    "        def __init__(\n",
    "            self,\n",
    "            cnn_model,\n",
    "            encoder,\n",
    "            decoder,\n",
    "            image_aug=None,\n",
    "        ):\n",
    "            super().__init__()\n",
    "            self.cnn_model = cnn_model\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "            self.acc_tracker = keras.metrics.Mean(name=\"accuracy\")\n",
    "            self.image_aug = image_aug\n",
    "\n",
    "        def calculate_loss(self, y_true, y_pred, mask):\n",
    "            loss = self.loss(y_true, y_pred)\n",
    "            mask = tf.cast(mask, dtype=loss.dtype)\n",
    "            loss *= mask\n",
    "            return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
    "\n",
    "        def calculate_accuracy(self, y_true, y_pred, mask):\n",
    "            accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))\n",
    "            accuracy = tf.math.logical_and(mask, accuracy)\n",
    "            accuracy = tf.cast(accuracy, dtype=tf.float32)\n",
    "            mask = tf.cast(mask, dtype=tf.float32)\n",
    "            return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)\n",
    "\n",
    "        def _compute_caption_loss_and_acc(self, img_embed, batch_seq, training=True):\n",
    "            # Process a single caption per image\n",
    "            encoder_out = self.encoder(img_embed, training=training)\n",
    "            batch_seq_inp = batch_seq[:, :-1]\n",
    "            batch_seq_true = batch_seq[:, 1:]\n",
    "            mask = tf.math.not_equal(batch_seq_true, 0)\n",
    "            batch_seq_pred = self.decoder(\n",
    "                batch_seq_inp, encoder_out, training=training, mask=mask\n",
    "            )\n",
    "            loss = self.calculate_loss(batch_seq_true, batch_seq_pred, mask)\n",
    "            acc = self.calculate_accuracy(batch_seq_true, batch_seq_pred, mask)\n",
    "            #print(\"Shape of encoder output:\", tf.shape(encoder_out))\n",
    "            #print(\"Shape of predicted sequence:\", tf.shape(batch_seq_pred))\n",
    "            return loss, acc\n",
    "\n",
    "        def train_step(self, batch_data):\n",
    "            batch_img, batch_seq = batch_data\n",
    "\n",
    "            # Print shapes of inputs\n",
    "            #print(\"Shape of batch_img:\", tf.shape(batch_img))\n",
    "            #print(\"Shape of batch_seq:\", tf.shape(batch_seq))\n",
    "\n",
    "            if self.image_aug:\n",
    "                batch_img = self.image_aug(batch_img)\n",
    "            img_embed = self.cnn_model(batch_img)\n",
    "            #print(\"Shape of img_embed:\", tf.shape(img_embed))\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss, acc = self._compute_caption_loss_and_acc(\n",
    "                    img_embed, batch_seq, training=True\n",
    "                )\n",
    "                self.loss_tracker.update_state(loss)\n",
    "                self.acc_tracker.update_state(acc)\n",
    "\n",
    "            train_vars = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "            grads = tape.gradient(loss, train_vars)\n",
    "            self.optimizer.apply_gradients(zip(grads, train_vars))\n",
    "\n",
    "            return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "\n",
    "        def test_step(self, batch_data):\n",
    "            batch_img, batch_seq = batch_data\n",
    "            # Print shapes of inputs\n",
    "            #print(\"Shape of batch_img (test):\", tf.shape(batch_img))\n",
    "            #print(\"Shape of batch_seq (test):\", tf.shape(batch_seq))\n",
    "            img_embed = self.cnn_model(batch_img)\n",
    "            #print(\"Shape of img_embed (test):\", tf.shape(img_embed))\n",
    "            loss, acc = self._compute_caption_loss_and_acc(\n",
    "                img_embed, batch_seq, training=False\n",
    "            )\n",
    "            self.loss_tracker.update_state(loss)\n",
    "            self.acc_tracker.update_state(acc)\n",
    "            return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            # We need to list our metrics here so the `reset_states()` can be\n",
    "            # called automatically.\n",
    "            return [self.loss_tracker, self.acc_tracker]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11699169-1f2a-4a26-9553-28950557934e",
   "metadata": {},
   "source": [
    "#### 11. Building Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b0187-fe0a-45e0-9b7d-23c25b552b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    cnn_model = get_cnn_model()\n",
    "    encoder = TransformerEncoderBlock(embed_dim=EMBED_DIM, dense_dim=FF_DIM, num_heads=1)\n",
    "    decoder = TransformerDecoderBlock(embed_dim=EMBED_DIM, ff_dim=FF_DIM, num_heads=2)\n",
    "    caption_model = ImageCaptioningModel(\n",
    "        cnn_model=cnn_model,\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438a923f-3d18-4697-945f-9e0b51816f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "\n",
    "    # Define the loss function\n",
    "    cross_entropy = keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=False,\n",
    "        reduction=tf.keras.losses.Reduction.NONE,\n",
    "    )\n",
    "\n",
    "    # EarlyStopping criteria\n",
    "    early_stopping = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "    \n",
    "\n",
    "    # Learning Rate Scheduler for the optimizer\n",
    "    class LRSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        def __init__(self, post_warmup_learning_rate, warmup_steps):\n",
    "            super().__init__()\n",
    "            self.post_warmup_learning_rate = post_warmup_learning_rate\n",
    "            self.warmup_steps = warmup_steps\n",
    "\n",
    "        def __call__(self, step):\n",
    "            global_step = tf.cast(step, tf.float32)\n",
    "            warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "            warmup_progress = global_step / warmup_steps\n",
    "            warmup_learning_rate = self.post_warmup_learning_rate * warmup_progress\n",
    "            return tf.cond(\n",
    "                global_step < warmup_steps,\n",
    "                lambda: warmup_learning_rate,\n",
    "                lambda: self.post_warmup_learning_rate,\n",
    "            )\n",
    "\n",
    "\n",
    "    # Create a learning rate schedule\n",
    "    num_train_steps = len(train_dataset) * EPOCHS\n",
    "    num_warmup_steps = num_train_steps // 15\n",
    "    lr_schedule = LRSchedule(post_warmup_learning_rate=1e-4, warmup_steps=num_warmup_steps)\n",
    "\n",
    "    # Compile the model\n",
    "    caption_model.compile(optimizer=tf.keras.optimizers.Adam(lr_schedule), loss=cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0521d7-0876-410d-b547-2af67180b4e9",
   "metadata": {},
   "source": [
    "#### 12. Training the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0e950f-6d6c-433a-bf87-b48dae4ad144",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    history = caption_model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=valid_dataset,\n",
    "        callbacks=[early_stopping],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db835888-5dc0-44c1-8e4b-92c0c1cbe1b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60858d85-ac98-4128-a27d-764d5816cb4c",
   "metadata": {},
   "source": [
    "#### 13. Loss and Accuracy Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370739ea-356d-4d38-bbde-73d7f3a0db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting loss and other metrics from the history object\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Update these lines with the correct keys\n",
    "acc = history.history['acc']  # Assuming 'acc' is the correct key for accuracy\n",
    "val_acc = history.history['val_acc']  # And 'val_acc' for validation accuracy\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# Plotting training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss, 'b-', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, acc, 'b-', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df5415-6a2d-48fd-bd70-a18029a4cb8a",
   "metadata": {},
   "source": [
    "#### 14. Test Dataset Predicted Outputs based on Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf83c9-efdc-4689-b47e-e066432f13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img, caption in test_dataset.take(1):\n",
    "    sample_img = img.numpy()[0]\n",
    "    print(\"Image shape:\", sample_img.shape)\n",
    "    plt.imshow(sample_img)\n",
    "    plt.title(\"Sample Image\")\n",
    "    plt.show()\n",
    "    image = tf.expand_dims(sample_img, 0)\n",
    "    print(\"Image shape after reshaping:\", image.shape)\n",
    "    cnn_img = caption_model.cnn_model(image)\n",
    "    encoded_img = caption_model.encoder(cnn_img, training=False)\n",
    "\n",
    "    vocab = vectorization.get_vocabulary()\n",
    "    index_lookup = dict(zip(range(len(vocab)), vocab))\n",
    "    max_decoded_sentence_length = SEQ_LENGTH - 1\n",
    "\n",
    "    sample_cap = caption.numpy()[0]\n",
    "    print(\"Caption shape:\", sample_cap.shape)\n",
    "    print(\"Caption Tokens:\", sample_cap)\n",
    "    \n",
    "    # Convert each token in sample_cap to the corresponding word and join them\n",
    "    actual_caption = \" \".join([index_lookup.get(token, '') for token in sample_cap if token != 0])\n",
    "    print(\"Ground Truth Caption:\", actual_caption)\n",
    "\n",
    "    decoded_caption = \"<start> \"\n",
    "\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
    "        mask = tf.math.not_equal(tokenized_caption, 0)\n",
    "        predictions = caption_model.decoder(\n",
    "            tokenized_caption, encoded_img, training=False, mask=mask\n",
    "        )\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = index_lookup[sampled_token_index]\n",
    "        if sampled_token == \"<end>\":\n",
    "            break\n",
    "        decoded_caption += \" \" + sampled_token\n",
    "\n",
    "    decoded_caption = decoded_caption.replace(\"<start> \", \"\")\n",
    "    decoded_caption = decoded_caption.replace(\" <end>\", \"\").strip()\n",
    "    print(\"Predicted Caption:\", decoded_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f893bb3-3545-493a-8c4c-27a5151f439c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "predicted_captions = []\n",
    "actual_captions = []\n",
    "\n",
    "\n",
    "# Assuming test_dataset is a tf.data.Dataset and each element is a tuple (image, caption, image_id)\n",
    "for img, caption in tqdm(test_dataset, desc=\"Processing Test Dataset\"):\n",
    "    for idx in range(img.shape[0]):  # Loop through the batch\n",
    "        sample_img = img.numpy()[idx]\n",
    "        image = tf.expand_dims(sample_img, 0)\n",
    "        cnn_img = caption_model.cnn_model(image)\n",
    "        encoded_img = caption_model.encoder(cnn_img, training=False)\n",
    "\n",
    "        vocab = vectorization.get_vocabulary()\n",
    "        index_lookup = dict(zip(range(len(vocab)), vocab))\n",
    "        max_decoded_sentence_length = SEQ_LENGTH - 1\n",
    "\n",
    "        sample_cap = caption.numpy()[idx]\n",
    "        actual_caption = \" \".join([index_lookup.get(token, '') for token in sample_cap if token != 0]).replace(\"<start>\", \"\").replace(\"<end>\", \"\").strip()\n",
    "        actual_captions.append(actual_caption)\n",
    "\n",
    "        decoded_caption = \"<start> \"\n",
    "        for i in range(max_decoded_sentence_length):\n",
    "            tokenized_caption = vectorization([decoded_caption])[:, :-1]\n",
    "            mask = tf.math.not_equal(tokenized_caption, 0)\n",
    "            predictions = caption_model.decoder(\n",
    "                tokenized_caption, encoded_img, training=False, mask=mask\n",
    "            )\n",
    "            sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "            sampled_token = index_lookup[sampled_token_index]\n",
    "            if sampled_token == \"<end>\":\n",
    "                break\n",
    "            decoded_caption += \" \" + sampled_token\n",
    "\n",
    "        decoded_caption = decoded_caption.replace(\"<start> \", \"\").replace(\"<end>\", \"\").strip()\n",
    "        predicted_captions.append(decoded_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355354e4-dd0e-40f2-b8d6-29d7579fb9d5",
   "metadata": {},
   "source": [
    "#### 15. Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc040c-959f-4cbc-bd66-97985ef848aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bleu(reference,prediction):\n",
    "    \"\"\"\n",
    "    Given a reference and prediction string, outputs the 1-gram,2-gram,3-gram and 4-gram bleu scores\n",
    "    \"\"\"\n",
    "    reference = [reference.split()] #should be in an array (cos of multiple references can be there here only 1)\n",
    "    prediction = prediction.split()\n",
    "    bleu1 = sentence_bleu(reference,prediction,weights = (1,0,0,0))\n",
    "    bleu2 = sentence_bleu(reference,prediction,weights = (0.5,0.5,0,0))\n",
    "    bleu3 = sentence_bleu(reference,prediction,weights = (0.33,0.33,0.33,0))\n",
    "    bleu4 = sentence_bleu(reference,prediction,weights = (0.25,0.25,0.25,0.25))\n",
    "\n",
    "    return bleu1,bleu2,bleu3,bleu4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb22423-0b9f-4f8b-9cd5-7654afae65b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "def mean_bleu(pred_ls, act_ls, **kwargs):\n",
    "\n",
    "    bleu1, bleu2, bleu3, bleu4 = [], [], [], []\n",
    "\n",
    "    for k in range (len(pred_ls)):\n",
    "        # Tokenize the true and predicted captions\n",
    "        true_tokens = act_ls[k]\n",
    "        #print (\"True: \",true_tokens)\n",
    "        predict_tokens = pred_ls[k]\n",
    "        #print (\"Predicted: \",predict_tokens)\n",
    "        \n",
    "        # Calculate BLEU scores\n",
    "        bleu1.append(sentence_bleu([true_tokens], predict_tokens, weights=(1, 0, 0, 0)))\n",
    "        bleu2.append(sentence_bleu([true_tokens], predict_tokens, weights=(0.5, 0.5, 0, 0)))\n",
    "        bleu3.append(sentence_bleu([true_tokens], predict_tokens, weights=(0.33, 0.33, 0.33, 0)))\n",
    "        bleu4.append(sentence_bleu([true_tokens], predict_tokens, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "    return np.array(bleu1).mean(), np.array(bleu2).mean(), np.array(bleu3).mean(), np.array(bleu4).mean()\n",
    "bleu1,bleu2,bleu3,bleu4 = mean_bleu(predicted_captions,actual_captions)\n",
    "print (\"Bleu Score 1-gram: \",bleu1)\n",
    "print (\"Bleu Score 2-gram: \",bleu2)\n",
    "print (\"Bleu Score 3-gram: \",bleu3)\n",
    "print (\"Bleu Score 4-gram: \",bleu4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70828816-4861-4d99-b7f6-e3aa343c69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "def calculate_rouge_scores(predictions, references):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(predictions, references, avg=True)\n",
    "    return scores\n",
    "\n",
    "rouge_scores = calculate_rouge_scores(predicted_captions, actual_captions)\n",
    "print(rouge_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d6017-4247-42f3-b7d0-b8700099956e",
   "metadata": {},
   "source": [
    "    ROUGE-1:\n",
    "        r: 19.85% (This means 19.85% of the words in the reference appear in the generated text)\n",
    "        p: 26.59% (This means 26.59% of the words in the generated text are also in the reference)\n",
    "        f: 21.08% (This is the F1-score, balancing precision and recall for unigrams)\n",
    "\n",
    "    ROUGE-2:\n",
    "        r: 6.10% (Lower recall for bigrams suggests less exact matches of word pairs)\n",
    "        p: 8.59% (Precision for bigrams is also lower, indicating less exact pairings of words)\n",
    "        f: 6.47% (The F1-score for bigrams is significantly lower than for unigrams)\n",
    "\n",
    "    ROUGE-L:\n",
    "        r: 18.00% (Recall for the longest common subsequence)\n",
    "        p: 23.81% (Precision for the longest common subsequence)\n",
    "        f: 18.98% (F1-score for the longest common subsequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59446885-8ce0-4853-ac4e-014111d68649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import meteor_score\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the captions\n",
    "# Assuming actual_captions and predicted_captions are lists of strings\n",
    "actual_captions_tokenized = [[act.split()] for act in actual_captions]  # List of lists of lists for references\n",
    "predicted_captions_tokenized = [pred.split() for pred in predicted_captions]  # List of lists for hypothesis\n",
    "\n",
    "# Calculate METEOR scores\n",
    "meteor_scores = [meteor_score(ref, pred) for ref, pred in zip(actual_captions_tokenized, predicted_captions_tokenized)]\n",
    "mean_meteor_score = np.mean(meteor_scores)\n",
    "\n",
    "print(f\"Mean METEOR score: {mean_meteor_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ebc56d-1f4b-4255-8926-0cb4731a9f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
