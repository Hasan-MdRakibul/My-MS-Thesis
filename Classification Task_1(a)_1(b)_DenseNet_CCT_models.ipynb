{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54195672-2da6-43bb-8160-7952bdf17b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ''\n",
    "import tensorflow as tf\n",
    "print(\"Num GPUs available: \",len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1b80d-05f7-42c9-8a19-6052712d107d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfed551-d34f-4cd9-9998-03b4867cf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import clear_session\n",
    "clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c8236-a576-4eca-a617-5f9856d077f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f941e9e-4bfd-40b6-8466-b5473736ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    #Dataset Preparation\n",
    "    #Loading MIAS, DDSM, INBreast, and VinDR Datasets\n",
    "    mias_df = pd.read_csv('D:/MIAS_all-mias/FINAL_mias.csv')\n",
    "    print (\"MIAS Dataset\")\n",
    "    print (mias_df.info())\n",
    "    print (\"Number of records: \",len(mias_df['labels']))\n",
    "    print (mias_df['labels'].value_counts())\n",
    "    print (\"--------------------------------------------\")\n",
    "    \n",
    "    inbreast_df = pd.read_csv(\"D:/InBreast/2023/InBreast/FINAL_inb.csv\")\n",
    "    print (\"INbreast Dataset\")\n",
    "    print (inbreast_df.info())\n",
    "    print (\"Number of records: \",len(inbreast_df['labels']))\n",
    "    print (inbreast_df['labels'].value_counts())\n",
    "    print (\"--------------------------------------------\")\n",
    "    \n",
    "    ddsm_df = pd.read_csv(\"D:/CBIS-DDSM/FINAL_ddsm.csv\")\n",
    "    print (\"DDSM Dataset\")\n",
    "    print (ddsm_df.info())\n",
    "    print (\"Number of records: \",len(ddsm_df['labels']))\n",
    "    print (ddsm_df['labels'].value_counts())\n",
    "    print (\"--------------------------------------------\")\n",
    "    \n",
    "    vin_df = pd.read_csv(\"D:/VinDRmammo/vindr/vindr/FINAL_vin.csv\")\n",
    "    print (\"VinDR-Mammo Dataset\")\n",
    "    print (vin_df.info())\n",
    "    print (\"Number of records: \",len(vin_df['labels']))\n",
    "    print (vin_df['labels'].value_counts())\n",
    "    print (\"--------------------------------------------\")\n",
    "    \n",
    "    # List of DataFrames to concatenate\n",
    "    dfs = [mias_df, inbreast_df, ddsm_df, vin_df]\n",
    "    \n",
    "    # Concatenate the DataFrames\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Display information about the merged dataset\n",
    "    print(\"Merged Dataset\")\n",
    "    print(merged_df.info())\n",
    "    print(\"Number of records: \", len(merged_df['labels']))\n",
    "    print(merged_df['labels'].value_counts())\n",
    "    print (\"------------------------------------------------\")\n",
    "    print (\"Sample Image Path: \",merged_df['image_path'][0])\n",
    "    print (\"Sample label: \", merged_df['labels'][0])\n",
    "    print ('###################################################')\n",
    "\n",
    "\n",
    "    # Step 1: Split into training (80%) and temp (20%)\n",
    "    train_df, temp_df = train_test_split(merged_df, test_size=0.2, random_state=42, stratify=merged_df['labels'])\n",
    "    \n",
    "    # Step 2: Split the temp into validation (75% of 20% -> 15% of total) and test (25% of 20% -> 5% of total)\n",
    "    valid_df, test_df = train_test_split(temp_df, test_size=0.25, random_state=42, stratify=temp_df['labels'])\n",
    "    \n",
    "    print(f\"Training set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(valid_df)}\")\n",
    "    print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7cc473-33dc-4cb6-b2da-103047788791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Directly use integers for classes\n",
    "class_labels = [0, 1]  # Corresponding to 'BENIGN': 0, 'MALIGNANT': 1\n",
    "\n",
    "# Convert string labels to integers\n",
    "train_labels_int = train_df['labels'].map({'BENIGN': 0, 'MALIGNANT': 1}).values\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced',\n",
    "                                     classes=class_labels,\n",
    "                                     y=train_labels_int)\n",
    "\n",
    "# Create a dictionary mapping class indices to their respective weights\n",
    "class_weights_dict = {class_label: weight for class_label, weight in zip(class_labels, class_weights)}\n",
    "\n",
    "print(\"Class Weights Dictionary:\", class_weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f6a76-6ed2-498b-a2b3-625ec52b0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_emb = True\n",
    "conv_layers = 2\n",
    "projection_dim = 128\n",
    "\n",
    "num_heads = 2\n",
    "transformer_units = [\n",
    "    projection_dim,\n",
    "    projection_dim,\n",
    "]\n",
    "transformer_layers = 2\n",
    "stochastic_depth_rate = 0.1\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 16\n",
    "num_epochs = 30\n",
    "image_size = 320\n",
    "input_shape = (image_size, image_size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f93e7-213a-4968-8c49-c93ca3a5a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def preprocess_image(image_path, label):\n",
    "        # Read the image from file\n",
    "        image = tf.io.read_file(image_path)\n",
    "        \n",
    "        # Dynamically decode image based on extension\n",
    "        def decode_jpeg(image):\n",
    "            return tf.image.decode_jpeg(image, channels=3)\n",
    "        \n",
    "        def decode_png(image):\n",
    "            return tf.image.decode_png(image, channels=3)\n",
    "        \n",
    "        # Use tf.cond to dynamically decide how to decode images\n",
    "        image = tf.cond(\n",
    "            tf.strings.regex_full_match(image_path, \".+\\.jpg\"),\n",
    "            lambda: decode_jpeg(image),\n",
    "            lambda: decode_png(image)\n",
    "        )\n",
    "        image = tf.image.resize(image, [image_size, image_size])\n",
    "        image = tf.cast(image, tf.float32) / 255.0  # Normalize image pixels to [0, 1]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "    def prepare_dataset(df, batch_size=32):\n",
    "        # Convert labels from string to integers (0 or 1)\n",
    "        labels = df['labels'].map({'BENIGN': 0, 'MALIGNANT': 1}).values\n",
    "        # Create a dataset from the image paths and labels\n",
    "        paths = df['image_path'].values\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "        # Map the preprocessing function to each element\n",
    "        dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        # Shuffle the dataset (Note: Only shuffle the training dataset)\n",
    "        # dataset = dataset.shuffle(buffer_size=len(df))\n",
    "        # Batch the dataset\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        # Prefetch to improve speed of input pipeline\n",
    "        dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "    \n",
    "    # Prepare the datasets\n",
    "    train_dataset = prepare_dataset(train_df, batch_size=batch_size)\n",
    "    valid_dataset = prepare_dataset(valid_df, batch_size=batch_size)\n",
    "    test_dataset = prepare_dataset(test_df, batch_size=batch_size)\n",
    "    \n",
    "    # Take one batch from the training dataset\n",
    "    for images, labels in train_dataset.take(1):\n",
    "        # Take the first image from this batch\n",
    "        sample_image = images[0]\n",
    "        sample_label = labels[0].numpy()\n",
    "        \n",
    "        # Plot the sample image\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        plt.imshow(sample_image)\n",
    "        plt.title(f\"Label: {sample_label}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    \n",
    "    def print_dataset_shapes(dataset, dataset_name):\n",
    "        for images, labels in dataset.take(1):  # Only take a single batch\n",
    "            print(f\"{dataset_name} - Images shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "    \n",
    "    print_dataset_shapes(train_dataset, \"Train\")\n",
    "    print_dataset_shapes(valid_dataset, \"Valid\")\n",
    "    print_dataset_shapes(test_dataset, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b0a59-7999-4ae6-a8ec-4e8e7ecfba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class CCTTokenizer(layers.Layer):\n",
    "        def __init__(\n",
    "            self,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            pooling_kernel_size=3,\n",
    "            pooling_stride=2,\n",
    "            num_conv_layers=conv_layers,\n",
    "            num_output_channels=[64, 128],\n",
    "            positional_emb=positional_emb,\n",
    "            **kwargs,\n",
    "        ):\n",
    "            super().__init__(**kwargs)\n",
    "    \n",
    "            # Debugging: print the initialization parameters\n",
    "            print(\"Initializing CCTTokenizer with parameters:\")\n",
    "            print(\"  kernel_size:\", kernel_size)\n",
    "            print(\"  stride:\", stride)\n",
    "            print(\"  padding:\", padding)\n",
    "            print(\"  pooling_kernel_size:\", pooling_kernel_size)\n",
    "            print(\"  pooling_stride:\", pooling_stride)\n",
    "            print(\"  num_conv_layers:\", num_conv_layers)\n",
    "            print(\"  num_output_channels:\", num_output_channels)\n",
    "    \n",
    "            self.conv_model = keras.Sequential()\n",
    "            for i in range(num_conv_layers):\n",
    "                self.conv_model.add(\n",
    "                    layers.Conv2D(\n",
    "                        num_output_channels[i],\n",
    "                        kernel_size,\n",
    "                        stride,\n",
    "                        padding=\"valid\",\n",
    "                        use_bias=False,\n",
    "                        activation=\"relu\",\n",
    "                        kernel_initializer=\"he_normal\",\n",
    "                    )\n",
    "                )\n",
    "                self.conv_model.add(layers.ZeroPadding2D(padding))\n",
    "                self.conv_model.add(\n",
    "                    layers.MaxPooling2D(pooling_kernel_size, pooling_stride, \"same\")\n",
    "                )\n",
    "    \n",
    "            self.positional_emb = positional_emb\n",
    "    \n",
    "        def call(self, images):\n",
    "            # Debugging: print the shape of the input images\n",
    "            print(\"Input shape/CCTTokenizer:\", images.shape)\n",
    "    \n",
    "            outputs = self.conv_model(images)\n",
    "    \n",
    "            # Debugging: print the shape of the output after the convolutional model\n",
    "            print(\"Output shape after conv_model:\", outputs.shape)\n",
    "    \n",
    "            reshaped = tf.reshape(\n",
    "                outputs,\n",
    "                (\n",
    "                    -1,\n",
    "                    tf.shape(outputs)[1] * tf.shape(outputs)[2],\n",
    "                    tf.shape(outputs)[-1],\n",
    "                ),\n",
    "            )\n",
    "    \n",
    "            # Debugging: print the shape after reshaping\n",
    "            print(\"Output shape after reshaping/CCTTokenizer:\", reshaped.shape)\n",
    "    \n",
    "            return reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0aa363-c43c-47ad-afc9-d14b6abca025",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class PositionEmbedding(keras.layers.Layer):\n",
    "        def __init__(self, sequence_length, initializer=\"glorot_uniform\", **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            if sequence_length is None:\n",
    "                raise ValueError(\"`sequence_length` must be an Integer, received `None`.\")\n",
    "            self.sequence_length = int(sequence_length)\n",
    "            self.initializer = keras.initializers.get(initializer)\n",
    "    \n",
    "        def get_config(self):\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"initializer\": keras.initializers.serialize(self.initializer),\n",
    "            })\n",
    "            return config\n",
    "    \n",
    "        def build(self, input_shape):\n",
    "            feature_size = input_shape[-1]\n",
    "            self.position_embeddings = self.add_weight(\n",
    "                name=\"embeddings\",\n",
    "                shape=[self.sequence_length, feature_size],\n",
    "                initializer=self.initializer,\n",
    "                trainable=True,\n",
    "            )\n",
    "    \n",
    "        def call(self, inputs, start_index=0):\n",
    "            print(\"Input shape/Positional Embedding:\", inputs.shape)\n",
    "            shape = tf.shape(inputs)\n",
    "            feature_length = shape[-1]\n",
    "            sequence_length = shape[-2]\n",
    "            position_embeddings = tf.convert_to_tensor(self.position_embeddings)\n",
    "            position_embeddings = tf.slice(position_embeddings, (start_index, 0), (sequence_length, feature_length))\n",
    "            print(\"Output shape/Positional Embedding:\", position_embeddings.shape)\n",
    "            return tf.broadcast_to(position_embeddings, shape)\n",
    "    \n",
    "        def compute_output_shape(self, input_shape):\n",
    "            return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674354b3-8449-417a-bdd4-6827565243cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class SequencePooling(layers.Layer):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.attention = layers.Dense(1)\n",
    "    \n",
    "    def call(self, x):\n",
    "        print(\"Input shape/Sequence Pooling:\", x.shape)  # Debugging: print the shape of the inputs\n",
    "        attention_weights = tf.nn.softmax(self.attention(x), axis=1)\n",
    "        print(\"Attention weights shape/Sequence Pooling:\", attention_weights.shape)  # Debugging: print the shape of the attention weights\n",
    "        attention_weights = tf.transpose(attention_weights, perm=[0, 2, 1])\n",
    "        weighted_representation = tf.matmul(attention_weights, x)\n",
    "        print(\"Weighted representation shape/Sequence Pooling:\", weighted_representation.shape)  # Debugging: print the shape of the weighted representation\n",
    "        return tf.squeeze(weighted_representation, axis=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c746a-4754-495a-8ea8-0edf6763e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    class StochasticDepth(layers.Layer):\n",
    "        def __init__(self, drop_prop, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.drop_prob = drop_prop\n",
    "    \n",
    "        def call(self, x, training=None):\n",
    "            if training and self.drop_prob > 0.0:\n",
    "                keep_prob = 1 - self.drop_prob\n",
    "                input_shape = tf.shape(x)\n",
    "    \n",
    "                # Debugging: Print the input shape\n",
    "                print(\"StochasticDepth input shape:\", x.shape)\n",
    "    \n",
    "                # Create a random tensor with the same shape as the input tensor\n",
    "                random_tensor = keep_prob + tf.random.uniform(input_shape, dtype=x.dtype)\n",
    "                binary_tensor = tf.floor(random_tensor)\n",
    "    \n",
    "                # Debugging: Print the shape of the binary tensor\n",
    "                print(\"Binary tensor shape:\", binary_tensor.shape)\n",
    "    \n",
    "                x = (x / keep_prob) * binary_tensor\n",
    "    \n",
    "                # Debugging: Print the output shape\n",
    "                print(\"StochasticDepth output shape:\", x.shape)\n",
    "    \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4054399a-cab6-4412-8aae-f96a3a9485b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def mlp(x, hidden_units, dropout_rate):\n",
    "        for idx, units in enumerate(hidden_units):\n",
    "            x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "            # Debugging: Print the shape of the output after the Dense layer\n",
    "            print(f\"Layer {idx+1} - Dense/mlp: Output shape {x.shape}\")\n",
    "    \n",
    "            x = layers.Dropout(dropout_rate)(x)\n",
    "            # Debugging: Print the shape of the output after the Dropout layer\n",
    "            print(f\"Layer {idx+1} - Dropout/mlp: Output shape {x.shape}\")\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908dee1-7569-4282-acc8-1b323e2c54ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    def create_cct_model(\n",
    "        image_size=image_size,\n",
    "        input_shape=input_shape,\n",
    "        num_heads=num_heads,\n",
    "        projection_dim=projection_dim,\n",
    "        transformer_units=transformer_units,\n",
    "        transformer_layers=transformer_layers,\n",
    "        positional_emb=True,\n",
    "        stochastic_depth_rate=0.1,\n",
    "    ):\n",
    "        inputs = layers.Input(input_shape)\n",
    "    \n",
    "        # Tokenize input images\n",
    "        cct_tokenizer = CCTTokenizer()\n",
    "        encoded_patches = cct_tokenizer(inputs)\n",
    "    \n",
    "        # Debugging: Print the shape of encoded patches\n",
    "        print(\"Encoded patches shape/create_cct_model:\", encoded_patches.shape)\n",
    "    \n",
    "        # Apply positional embedding if enabled\n",
    "        if positional_emb:\n",
    "            sequence_length = encoded_patches.shape[1]\n",
    "            encoded_patches += PositionEmbedding(sequence_length=sequence_length)(\n",
    "                encoded_patches\n",
    "            )\n",
    "    \n",
    "        # Calculate Stochastic Depth probabilities\n",
    "        dpr = [x for x in np.linspace(0, stochastic_depth_rate, transformer_layers)]\n",
    "    \n",
    "        # Create multiple layers of the Transformer block\n",
    "        for i in range(transformer_layers):\n",
    "            # Layer normalization 1\n",
    "            x1 = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "    \n",
    "            # Multi-head attention\n",
    "            attention_output = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "            )(x1, x1)\n",
    "    \n",
    "            # Debugging: Print the shape after multi-head attention\n",
    "            print(f\"Layer {i+1} - Multi-head attention output shape/create_cct_model: {attention_output.shape}\")\n",
    "    \n",
    "            # Skip connection 1\n",
    "            attention_output = StochasticDepth(dpr[i])(attention_output)\n",
    "            x2 = layers.Add()([attention_output, encoded_patches])\n",
    "    \n",
    "            # Layer normalization 2\n",
    "            x3 = layers.LayerNormalization(epsilon=1e-5)(x2)\n",
    "    \n",
    "            # MLP\n",
    "            x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "    \n",
    "            # Debugging: Print the shape after MLP\n",
    "            print(f\"Layer {i+1} - MLP output shape/create_cct_model: {x3.shape}\")\n",
    "    \n",
    "            # Skip connection 2\n",
    "            x3 = StochasticDepth(dpr[i])(x3)\n",
    "            encoded_patches = layers.Add()([x3, x2])\n",
    "    \n",
    "        # Apply sequence pooling or global average pooling\n",
    "        representation = layers.LayerNormalization(epsilon=1e-5)(encoded_patches)\n",
    "        weighted_representation = SequencePooling()(representation)\n",
    "        representation = layers.GlobalAveragePooling1D()(weighted_representation)  # Aggregate over tokens\n",
    "    \n",
    "        # Final classification layer\n",
    "        logits = layers.Dense(1, activation='sigmoid')(representation)\n",
    "    \n",
    "        # Debugging: Print the shape of final output logits\n",
    "        print(\"Final output logits shape/create_cct_model:\", logits.shape)\n",
    "    \n",
    "        # Create the Keras model\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=logits)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e2c356-6829-4537-be69-5a32a631070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def run_experiment(model):\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='binary_crossentropy',  # Updated for binary classification\n",
    "        metrics=[\n",
    "            keras.metrics.BinaryAccuracy(name=\"accuracy\"),  # Updated for binary classification\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    checkpoint_filepath = \"/tmp/checkpoint.weights.h5\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=valid_dataset,\n",
    "        class_weight=class_weights_dict,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        verbose = 1,\n",
    "    )\n",
    "\n",
    "\n",
    "    # Uncomment and adjust the following code if you have a test dataset\n",
    "    # model.load_weights(checkpoint_filepath)\n",
    "    # _, accuracy = model.evaluate(x_test, y_test)\n",
    "    # print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "cct_model = create_cct_model()\n",
    "history = run_experiment(cct_model)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfee485d-1d2b-48f4-b000-c3ae22490281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC, Precision, Recall\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "\n",
    "with strategy.scope():\n",
    "    def run_experiment(model):\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                BinaryAccuracy(name=\"accuracy\"),\n",
    "                AUC(name=\"auc\"),\n",
    "                Precision(name=\"precision\"),\n",
    "                Recall(name=\"recall\"),\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "        checkpoint_filepath = \"D:/Models and Results/CCT_large.h5\"\n",
    "        checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_filepath,\n",
    "            monitor=\"val_auc\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "        )\n",
    "    \n",
    "        early_stopping_callback = EarlyStopping(\n",
    "            monitor=\"val_loss\",  # You can change this to \"val_auc\" or another metric\n",
    "            patience=10,  # Number of epochs with no improvement after which training will be stopped\n",
    "            restore_best_weights=True,  # Whether to restore model weights from the epoch with the best value of the monitored quantity\n",
    "        )\n",
    "    \n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=num_epochs,\n",
    "            validation_data=valid_dataset,\n",
    "            class_weight=class_weights_dict,\n",
    "            callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "            verbose=1,\n",
    "        )\n",
    "    \n",
    "        # Load the best model weights\n",
    "        # model.load_weights(checkpoint_filepath)\n",
    "        \n",
    "        # Evaluate the model on the test dataset\n",
    "        test_metrics = model.evaluate(test_dataset, return_dict=True, verbose=1)\n",
    "        \n",
    "        # To calculate F1 Score, we need to get predictions for the test dataset\n",
    "        y_pred = model.predict(test_dataset)\n",
    "        # Assuming your model outputs probabilities for class 1 (MALIGNANT), and labels are binary\n",
    "        y_pred_binary = np.round(y_pred).astype(int)  # Convert probabilities to binary predictions\n",
    "        # Extract true labels from the test_dataset\n",
    "        y_true = np.concatenate([y[1].numpy() for y in test_dataset])\n",
    "        \n",
    "        # Calculate F1 Score\n",
    "        f1 = f1_score(y_true, y_pred_binary)\n",
    "        test_metrics['f1_score'] = f1\n",
    "        \n",
    "        # Print all test metrics\n",
    "        for metric, value in test_metrics.items():\n",
    "            print(f\"{metric}: {value}\")\n",
    "        \n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d495b-20f5-4ad7-add8-2d577da2004b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    cct_model = create_cct_model()\n",
    "    history = run_experiment(cct_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58149bfd-bb55-4ff7-afaf-e08b51983af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Training and Validation Accuracy and Loss\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633d6c7-16d8-40fa-9007-a405105dcd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Additional Metrics (e.g., AUC, Precision, Recall)\n",
    "# Plot training & validation AUC\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['auc'])\n",
    "plt.plot(history.history['val_auc'])\n",
    "plt.title('Model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "# Plot training & validation Precision\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['precision'])\n",
    "plt.plot(history.history['val_precision'])\n",
    "plt.title('Model Precision')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "# Plot training & validation Recall\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['recall'])\n",
    "plt.plot(history.history['val_recall'])\n",
    "plt.title('Model Recall')\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969fe6e-657b-49c1-b67f-fb1a3e79a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_metrics is a dictionary containing the test results\n",
    "test_metrics = cct_model.evaluate(test_dataset, return_dict=True)\n",
    "\n",
    "# Print each metric\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db1c943-9a60-46ce-b36d-7e63bf6773cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_dataset is properly batched\n",
    "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "y_pred = cct_model.predict(test_dataset, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f9a2c-57f1-4d55-bd5f-c3119dc16aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "\n",
    "auc = AUC()\n",
    "precision = Precision()\n",
    "recall = Recall()\n",
    "\n",
    "# Update state with true labels and predicted probabilities\n",
    "auc.update_state(y_true, y_pred)\n",
    "precision.update_state(y_true, np.round(y_pred))\n",
    "recall.update_state(y_true, np.round(y_pred))\n",
    "\n",
    "# Finally, get the result\n",
    "auc_result = auc.result().numpy()\n",
    "precision_result = precision.result().numpy()\n",
    "recall_result = recall.result().numpy()\n",
    "\n",
    "print(f\"AUC: {auc_result}\")\n",
    "print(f\"Precision: {precision_result}\")\n",
    "print(f\"Recall: {recall_result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb9fdd-ca4d-4882-b6e0-c39553f52f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d918de-7635-4f19-861c-dfa80549d964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae222a4b-c4ea-4b95-bc2c-fd4bbb7044a8",
   "metadata": {},
   "source": [
    "## DenseNet201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5f13d-551f-44f7-a269-04657b82dbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "def build_densenet(NUM_CLASSES):\n",
    "    inputs = layers.Input(shape=(image_size, image_size, 3))\n",
    "    model = tf.keras.applications.densenet.DenseNet201(include_top=False, input_tensor=inputs, weights=\"imagenet\")\n",
    "\n",
    "    # Freeze the pretrained weights\n",
    "    model.trainable = False\n",
    "\n",
    "    # Rebuild top\n",
    "    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    top_dropout_rate = 0.2\n",
    "    x = layers.Dropout(top_dropout_rate, name=\"top_dropout\")(x)\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation=\"sigmoid\", name=\"pred\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name=\"DenseNet201\")\n",
    "    return model\n",
    "\n",
    "dense_model = build_densenet(NUM_CLASSES=1)\n",
    "\n",
    "\n",
    "METRICS = [ \n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    tf.keras.metrics.AUC(name='prc', curve='PR'),\n",
    "]\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        return lr * 0.5\n",
    "    return lr\n",
    "\n",
    "learning_rate_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Create Adam optimizer with weight decay.\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-4)\n",
    "\n",
    "# Compile the model.\n",
    "dense_model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=METRICS,\n",
    ")\n",
    "\n",
    "\n",
    "# Create a learning rate scheduler callback.\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.5, patience=7\n",
    ")\n",
    "\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# ModelCheckpoint callback to save the best model\n",
    "checkpoint_filepath = 'D:/Models and Results/DenseNet201_thesis.h5'\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fa3f48-5cc6-4c50-9dbf-a64e92c8547b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model.\n",
    "dense_history = dense_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset, \n",
    "    class_weight=class_weights_dict,\n",
    "    batch_size=batch_size,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, reduce_lr, model_checkpoint, learning_rate_scheduler],\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4506161d-db23-47df-8539-7770a6dba362",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Training and Validation Accuracy and Loss\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(dense_history.history['accuracy'])\n",
    "plt.plot(dense_history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(dense_history.history['loss'])\n",
    "plt.plot(dense_history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41baecf9-3ec6-43d0-98f1-0b1ca4ddd28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Additional Metrics (e.g., AUC, Precision, Recall)\n",
    "# Plot training & validation AUC\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(dense_history.history['auc'])\n",
    "plt.plot(dense_history.history['val_auc'])\n",
    "plt.title('Model AUC')\n",
    "plt.ylabel('AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "# Plot training & validation Precision\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(dense_history.history['precision'])\n",
    "plt.plot(dense_history.history['val_precision'])\n",
    "plt.title('Model Precision')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "# Plot training & validation Recall\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(dense_history.history['recall'])\n",
    "plt.plot(dense_history.history['val_recall'])\n",
    "plt.title('Model Recall')\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0699406e-c0ec-42b4-aee8-6209678f5863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_metrics is a dictionary containing the test results\n",
    "test_metrics = dense_model.evaluate(test_dataset, return_dict=True)\n",
    "\n",
    "# Print each metric\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0bec47-3b23-42fe-bdcb-5dbe29ab235c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming test_dataset is properly batched\n",
    "y_true = np.concatenate([y for x, y in test_dataset], axis=0)\n",
    "y_pred = dense_model.predict(test_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6441dfe7-e476-413b-bbf1-8f85f1c138c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import AUC, Precision, Recall\n",
    "\n",
    "auc = AUC()\n",
    "precision = Precision()\n",
    "recall = Recall()\n",
    "\n",
    "# Update state with true labels and predicted probabilities\n",
    "auc.update_state(y_true, y_pred)\n",
    "precision.update_state(y_true, np.round(y_pred))\n",
    "recall.update_state(y_true, np.round(y_pred))\n",
    "\n",
    "# Finally, get the result\n",
    "auc_result = auc.result().numpy()\n",
    "precision_result = precision.result().numpy()\n",
    "recall_result = recall.result().numpy()\n",
    "\n",
    "print(f\"AUC: {auc_result}\")\n",
    "print(f\"Precision: {precision_result}\")\n",
    "print(f\"Recall: {recall_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5c5db-bc47-4a8d-8b61-364d6c6293cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdff914-0b22-46b6-aba1-45d8bd4b3500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d86cb1-902c-4659-a7c1-1905952b7184",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_binary = np.round(y_pred).astype(int)\n",
    "f1 = f1_score(y_true, y_pred_binary)\n",
    "test_metrics['f1_score'] = f1\n",
    "print ('F1 score for DenseNet201: ',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1db4c6-60ed-4a27-9539-33fe5d7a5ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
