# My-MS-Thesis
Transformer and Convolutional Neural Network based Hybrid Approaches in Medical Image Classification, Caption Generation, and Retrieval Processes

When examining Convolutional Neural Networks (CNNs) and Transformers in medical image processing, noticeable architectural differences are evident. CNNs use convolutional layers to analyze spatial structures in images, extracting local details and combining them to build a holistic understanding of the image. Transformers in the visual domain use self-attention mechanisms to analyze connections between different parts of an image at the same time, catching distant interconnections and contextual details. The combined use of CNNs and Transformers is a powerful approach that leverages the benefits of both structures to improve efficiency in various medical image processing jobs. This thesis explores the efficacy of hybrid methods in three areas of medical image analysis: classification, caption generation, and content-based image retrieval. For classification, this study uses the breast mammograms labeled with benign or malignant from CBIS-DDSM, INBreast, MIAS, and VinDr-Mammo datasets. The pre-trained DenseNet201 is the CNN-based method used, although the Compact Convolution Transformer (CCT) is employed as a hybrid approach for image classification. The evaluation measures of the hybrid technique outperform the CNN-based model, showing higher accuracy (0.7116), reduced loss (0.5658), improved precision (0.6000), increased recall (0.7317), better AUC (0.7865), and a higher F1 score (0.6593). In generating captions, the DenseNet121-Transformer hybrid model performs better than the DenseNet121-LSTM model for the ImageCLEF 2023 dataset. It achieves higher scores across BLEU-1, 2, 3, and 4, with values of 0.411, 0.301, 0.220, and 0.169, respectively. Finally for the retrieval task, the CLIP technique, with Xception and BERT as image and text encoders, respectively, surpasses the ResNet50-Cosine Similarity based approach in the image retrieval evaluation metric. The hybrid model achieves an accuracy of 32.41% using the IU Chest X-Ray and MIMIC-CXR datasets within the top 10 retrieval results. This thesis rigorously examines the hybrid technique and finds its effectiveness over the CNNs in those three key areas of medical image analysis.

