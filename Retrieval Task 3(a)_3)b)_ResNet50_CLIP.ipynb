{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaae500-d7e6-4120-9409-a17297fb132f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "def configure_gpus():\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(f\"{len(gpus)} Physical GPU(s), {len(logical_gpus)} Logical GPU(s) configured.\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"RuntimeError in configuring GPUs: {e}\")\n",
    "    else:\n",
    "        print(\"No GPU is available.\")\n",
    "\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "def check_jupyter_notebook():\n",
    "    try:\n",
    "        cfg = get_ipython().config \n",
    "        print(\"Jupyter Notebook environment detected. Configuring...\")\n",
    "        cfg.NotebookApp.iopub_msg_rate_limit = 10000.0\n",
    "        cfg.NotebookApp.rate_limit_window = 5.0\n",
    "    except NameError:\n",
    "        print(\"Not running in a Jupyter Notebook environment.\")\n",
    "\n",
    "# Clear TensorFlow session and suppress warnings\n",
    "tf.keras.backend.clear_session()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# GPU configuration and seed setting\n",
    "configure_gpus()\n",
    "seed_everything()\n",
    "\n",
    "# Check if running in Jupyter Notebook and configure\n",
    "check_jupyter_notebook()\n",
    "\n",
    "# Initialize TensorFlow distributed strategy\n",
    "# strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987df2f7-64be-4ff2-befd-0aa67fd03be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from glob import glob\n",
    "import PIL\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow_hub as hub\n",
    "tf.get_logger().setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800375e-da34-40b4-aa3e-be5e7c9c8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text  # This line will fail if TensorFlow Text is not correctly installed\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "print(tf.__version__)\n",
    "print(text.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd56e9f1-e70a-43b2-b480-22b88255cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = 'NLMCXR_reports/ecgen-radiology'\n",
    "image_data = []\n",
    "\n",
    "# Loop through all XML files in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.xml'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        abstract_texts = root.findall('.//Abstract/AbstractText')\n",
    "        abstract_info = {ab.get('Label').lower(): ab.text for ab in abstract_texts if ab.get('Label')}\n",
    "\n",
    "        # Go through each 'parentImage' element\n",
    "        for parent_image in root.findall('.//parentImage'):\n",
    "            image_id = parent_image.get('id')\n",
    "            caption = parent_image.find('.//caption').text if parent_image.find('.//caption') is not None else None\n",
    "\n",
    "            # Add to the list as a dictionary\n",
    "            image_data.append({\n",
    "                'filename': filename,\n",
    "                'image_id': image_id,\n",
    "                'caption': caption,\n",
    "                'comparison': abstract_info.get('comparison', ''),\n",
    "                'indication': abstract_info.get('indication', ''),\n",
    "                'findings': abstract_info.get('findings', ''),\n",
    "                'impression': abstract_info.get('impression', '')\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(image_data)\n",
    "print(df)\n",
    "df.to_csv('image_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844e414-ca98-4d7e-9e3a-afc88982c6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"NLMCXR_png/\"\n",
    "images = glob(image_path + \"*.png\")\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f52389-7e4c-4468-a7c6-de9b4335a4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    plt.figure()\n",
    "    image = cv2.imread(images[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1381e250-60e5-4d73-86dd-39efc28cffb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = df\n",
    "data = {}\n",
    "for i in range(len(reports)):\n",
    "    filename = reports.loc[i, 'image_id']\n",
    "    caption = reports.loc[i, 'impression']\n",
    "    if filename not in data:\n",
    "        data[filename] = []\n",
    "    if isinstance(caption, str) and re.match(r'^\\d+\\.', caption):\n",
    "        data[filename].append(caption.split('. ')[1])\n",
    "    else:\n",
    "        if data[filename]:\n",
    "            data[filename][-1] += \" \" + caption\n",
    "        else:\n",
    "            data[filename].append(caption)\n",
    "list(data.items())[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6975b177-6fb8-4477-8254-9580772464d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_data(data):\n",
    "    dict_2 = dict()\n",
    "    for key, value in data.items():\n",
    "        for i in range(len(value)):\n",
    "            lines = \"\"\n",
    "            line1 = value[i]\n",
    "            if isinstance(line1, str):\n",
    "                for j in line1.split():\n",
    "                    if len(j) < 2:\n",
    "                        continue\n",
    "                    j = j.lower()\n",
    "                    lines += j + \" \"\n",
    "                if key not in dict_2:\n",
    "                    dict_2[key] = list()\n",
    "                dict_2[key].append(lines)\n",
    "    return dict_2\n",
    "\n",
    "data2 = cleanse_data(data)\n",
    "print(len(data2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e4aac-fe92-4167-b331-c667d7c11b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the following into a vocabulary of words and calculate the total words\n",
    "\n",
    "def vocabulary(data2):\n",
    "    all_desc = set()\n",
    "    for key in data2.keys():\n",
    "        [all_desc.update(d.split()) for d in data2[key]]\n",
    "    return all_desc\n",
    "\n",
    "# summarize vocabulary\n",
    "vocabulary_data = vocabulary(data2)\n",
    "print(len(vocabulary_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2abe28a-eac7-4097-a106-9a85cb84976b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(data2, filename):\n",
    "    lines = list()\n",
    "    for key, value in data2.items():\n",
    "        for desc in value:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "save_dict(data2, 'captions1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d30101-fda9-4348-a89b-7ccf21fdc66a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b000c815-9e56-4cd5-b239-f1fc610647b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_to_caption = {}\n",
    "# Image folder path\n",
    "image_folder = 'NLMCXR_png/'  # Adjusted to a hypothetical path for demonstration\n",
    "\n",
    "# 1. Pre-process captions\n",
    "def clean_caption(caption):\n",
    "    # Convert to lowercase, remove punctuation, etc.\n",
    "    return caption.lower().strip()\n",
    "\n",
    "# Apply cleaning\n",
    "for img_name, captions in data2.items():\n",
    "    data2[img_name] = [clean_caption(cap) for cap in captions]\n",
    "\n",
    "# 2. Prepare image paths and captions\n",
    "for img_name, cap_list in data2.items():\n",
    "    for cap in cap_list:\n",
    "        image_path = os.path.join(image_folder, img_name + '.png')\n",
    "        # Initialize a list for the image_path if it doesn't exist yet\n",
    "        if image_path not in image_path_to_caption:\n",
    "            image_path_to_caption[image_path] = []\n",
    "        # Use 'cap' variable which holds the cleaned caption\n",
    "        image_path_to_caption[image_path].append(cap)\n",
    "\n",
    "image_paths = list(image_path_to_caption.keys())\n",
    "print(f\"Number of images: {len(image_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d5607b-9145-47ed-8049-95be383d1298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8bb72c-d5a3-4937-b940-f7d374d747a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_dir = 'tfrecords/'\n",
    "train_size = 5000\n",
    "valid_size = 2418\n",
    "captions_per_image = 1\n",
    "images_per_file = 500\n",
    "\n",
    "train_image_paths = image_paths[:train_size]\n",
    "num_train_files = int(np.ceil(train_size / images_per_file))\n",
    "train_files_prefix = os.path.join(tfrecords_dir, \"train\")\n",
    "\n",
    "valid_image_paths = image_paths[-valid_size:]\n",
    "num_valid_files = int(np.ceil(valid_size / images_per_file)) ##Need to define 418 images\n",
    "valid_files_prefix = os.path.join(tfrecords_dir, \"valid\")\n",
    "\n",
    "tf.io.gfile.makedirs(tfrecords_dir)\n",
    "\n",
    "\n",
    "def bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def create_example(image_path, caption):\n",
    "    feature = {\n",
    "        \"caption\": bytes_feature(caption.encode()),\n",
    "        \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def write_tfrecords(file_name, image_paths):\n",
    "    caption_list = []\n",
    "    image_path_list = []\n",
    "    for image_path in image_paths:\n",
    "        captions = image_path_to_caption[image_path][:captions_per_image]\n",
    "        caption_list.extend(captions)\n",
    "        image_path_list.extend([image_path] * len(captions))\n",
    "\n",
    "    with tf.io.TFRecordWriter(file_name) as writer:\n",
    "        for example_idx in range(len(image_path_list)):\n",
    "            example = create_example(\n",
    "                image_path_list[example_idx], caption_list[example_idx]\n",
    "            )\n",
    "            writer.write(example.SerializeToString())\n",
    "    return example_idx + 1\n",
    "\n",
    "\n",
    "def write_data(image_paths, num_files, files_prefix):\n",
    "    example_counter = 0\n",
    "    for file_idx in tqdm(range(num_files)):\n",
    "        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n",
    "        start_idx = images_per_file * file_idx\n",
    "        end_idx = start_idx + images_per_file\n",
    "        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n",
    "    return example_counter\n",
    "\n",
    "\n",
    "train_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\n",
    "print(f\"{train_example_count} training examples were written to tfrecord files.\")\n",
    "\n",
    "valid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\n",
    "print(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb52efe-bdc6-4c42-a136-f64f7b531d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_description = {\n",
    "    \"caption\": tf.io.FixedLenFeature([], tf.string),\n",
    "    \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "\n",
    "def read_example(example):\n",
    "    features = tf.io.parse_single_example(example, feature_description)\n",
    "    raw_image = features.pop(\"raw_image\")\n",
    "    features[\"image\"] = tf.image.resize(\n",
    "        tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "def get_dataset(file_pattern, batch_size):\n",
    "\n",
    "    return (\n",
    "        tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n",
    "        .map(\n",
    "            read_example,\n",
    "            num_parallel_calls=tf.data.AUTOTUNE,\n",
    "            deterministic=False,\n",
    "        )\n",
    "        .shuffle(batch_size * 10)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        .batch(batch_size)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c30d9-29f5-4a52-a29a-76c9e0c5b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Image folder path\n",
    "image_folder = 'NLMCXR_png/'  # Adjusted to a hypothetical path for demonstration\n",
    "\n",
    "# 1. Pre-process captions\n",
    "def clean_caption(caption):\n",
    "    # Convert to lowercase, remove punctuation, etc.\n",
    "    return caption.lower().strip()\n",
    "\n",
    "# Apply cleaning\n",
    "for img_name, captions in data2.items():\n",
    "    data2[img_name] = [clean_caption(cap) for cap in captions]\n",
    "\n",
    "# 2. Prepare image paths and captions\n",
    "image_paths = []\n",
    "captions = []\n",
    "for img_name, cap_list in data2.items():\n",
    "    for cap in cap_list:\n",
    "        image_paths.append(os.path.join(image_folder, img_name + '.png'))\n",
    "        captions.append(cap)\n",
    "\n",
    "# 3. Tokenization and Vectorization\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(captions)\n",
    "seqs = tokenizer.texts_to_sequences(captions)\n",
    "max_length = max(len(seq) for seq in seqs)\n",
    "cap_vector = pad_sequences(seqs, maxlen=max_length, padding='post')\n",
    "\n",
    "# Now, split the dataset\n",
    "image_paths_train_val, image_paths_test, cap_vector_train_val, cap_vector_test = train_test_split(\n",
    "    image_paths, cap_vector, test_size=0.05, random_state=42)\n",
    "\n",
    "image_paths_train, image_paths_val, cap_vector_train, cap_vector_val = train_test_split(\n",
    "    image_paths_train_val, cap_vector_train_val, test_size=0.15/(0.80+0.15), random_state=42)\n",
    "\n",
    "# Verify the distribution\n",
    "len_train, len_val, len_test = len(image_paths_train), len(image_paths_val), len(image_paths_test)\n",
    "\n",
    "print(\"Train samples:\", len_train)\n",
    "print(\"Validation samples:\", len_val)\n",
    "print(\"Test samples:\", len_test)\n",
    "\n",
    "# 5. Create tf.data.Dataset\n",
    "def map_func(img_path, cap):\n",
    "    img = tf.io.read_file(img_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = img / 255.0\n",
    "    return img, cap\n",
    "\n",
    "batch_size = 32  # Example batch size\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((image_paths_train, cap_vector_train))\n",
    "train_dataset = train_dataset.map(map_func).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((image_paths_val, cap_vector_val))\n",
    "val_dataset = val_dataset.map(map_func).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Display shapes and a sample image with caption\n",
    "for img_batch, cap_batch in train_dataset.take(1):\n",
    "    sample_img = img_batch[0]\n",
    "    sample_cap = cap_batch[0]\n",
    "    plt.imshow(sample_img)\n",
    "    plt.title(\"Sample Image\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Image batch shape:\", img_batch.shape)\n",
    "    print(\"Caption batch shape:\", cap_batch.shape)\n",
    "    \n",
    "    # Convert first caption back to words\n",
    "    sample_cap_words = tokenizer.sequences_to_texts([sample_cap.numpy()])[0]\n",
    "    print(\"Sample Caption:\", sample_cap_words)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a645c3c-3295-4064-8d77-99f46c4da15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_embeddings(\n",
    "    embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "):\n",
    "    projected_embeddings = tf.keras.layers.Dense(units=projection_dims)(embeddings)\n",
    "    for _ in range(num_projection_layers):\n",
    "        x = tf.nn.gelu(projected_embeddings)\n",
    "        x = tf.keras.layers.Dense(projection_dims)(x)\n",
    "        x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "        x = tf.keras.layers.Add()([projected_embeddings, x])\n",
    "        projected_embeddings = tf.keras.layers.LayerNormalization()(x)\n",
    "    return projected_embeddings\n",
    "\n",
    "def create_vision_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the pre-trained Xception model to be used as the base encoder.\n",
    "    xception = keras.applications.Xception(\n",
    "        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n",
    "    )\n",
    "    # Set the trainability of the base encoder.\n",
    "    for layer in xception.layers:\n",
    "        layer.trainable = trainable\n",
    "    # Receive the images as inputs.\n",
    "    inputs = tf.keras.layers.Input(shape=(299, 299, 3), name=\"image_input\")\n",
    "    # Preprocess the input image.\n",
    "    xception_input = tf.keras.applications.xception.preprocess_input(inputs)\n",
    "    # Generate the embeddings for the images using the xception model.\n",
    "    embeddings = xception(xception_input)\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the vision encoder model.\n",
    "    return keras.Model(inputs, outputs, name=\"vision_encoder\")\n",
    "\n",
    "def create_text_encoder(\n",
    "    num_projection_layers, projection_dims, dropout_rate, trainable=False\n",
    "):\n",
    "    # Load the BERT preprocessing module.\n",
    "    preprocess = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/2\",\n",
    "        name=\"text_preprocessing\",\n",
    "    )\n",
    "    # Load the pre-trained BERT model to be used as the base encoder.\n",
    "    bert = hub.KerasLayer(\n",
    "        \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\",\n",
    "        trainable=trainable,  # Correctly passing the trainable parameter\n",
    "        name=\"bert\",  # Correctly specifying the name argument\n",
    "    )\n",
    "    # Receive the text as inputs.\n",
    "    inputs = tf.keras.layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n",
    "    # Preprocess the text.\n",
    "    bert_inputs = preprocess(inputs)\n",
    "    # Generate embeddings for the preprocessed text using the BERT model.\n",
    "    embeddings = bert(bert_inputs)[\"pooled_output\"]\n",
    "    # Project the embeddings produced by the model.\n",
    "    outputs = project_embeddings(\n",
    "        embeddings, num_projection_layers, projection_dims, dropout_rate\n",
    "    )\n",
    "    # Create the text encoder model.\n",
    "    return tf.keras.Model(inputs, outputs, name=\"text_encoder\")\n",
    "\n",
    "\n",
    "\n",
    "class DualEncoder(keras.Model):\n",
    "    def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.temperature = temperature\n",
    "        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker]\n",
    "\n",
    "    def call(self, features, training=False):\n",
    "        # Place each encoder on a separate GPU (if available).\n",
    "        # TF will fallback on available devices if there are fewer than 2 GPUs.\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            # Get the embeddings for the captions.\n",
    "            caption_embeddings = text_encoder(features[\"caption\"], training=training)\n",
    "        with tf.device(\"/gpu:1\"):\n",
    "            # Get the embeddings for the images.\n",
    "            image_embeddings = vision_encoder(features[\"image\"], training=training)\n",
    "        return caption_embeddings, image_embeddings\n",
    "\n",
    "    def compute_loss(self, caption_embeddings, image_embeddings):\n",
    "        # logits[i][j] is the dot_similarity(caption_i, image_j).\n",
    "        logits = (\n",
    "            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n",
    "            / self.temperature\n",
    "        )\n",
    "        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n",
    "        images_similarity = tf.matmul(\n",
    "            image_embeddings, image_embeddings, transpose_b=True\n",
    "        )\n",
    "        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n",
    "        captions_similarity = tf.matmul(\n",
    "            caption_embeddings, caption_embeddings, transpose_b=True\n",
    "        )\n",
    "        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n",
    "        targets = keras.activations.softmax(\n",
    "            (captions_similarity + images_similarity) / (2 * self.temperature)\n",
    "        )\n",
    "        # Compute the loss for the captions using crossentropy\n",
    "        captions_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=targets, y_pred=logits, from_logits=True\n",
    "        )\n",
    "        # Compute the loss for the images using crossentropy\n",
    "        images_loss = keras.losses.categorical_crossentropy(\n",
    "            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n",
    "        )\n",
    "        # Return the mean of the loss over the batch.\n",
    "        return (captions_loss + images_loss) / 2\n",
    "\n",
    "    def train_step(self, features):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass\n",
    "            caption_embeddings, image_embeddings = self(features, training=True)\n",
    "            loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        # Backward pass\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        # Monitor loss\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}\n",
    "\n",
    "    def test_step(self, features):\n",
    "        caption_embeddings, image_embeddings = self(features, training=False)\n",
    "        loss = self.compute_loss(caption_embeddings, image_embeddings)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        return {\"loss\": self.loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6763514-3a90-4934-9400-c8d1de1fe7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 64\n",
    "vision_encoder = create_vision_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "text_encoder = create_text_encoder(\n",
    "    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n",
    ")\n",
    "dual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\n",
    "dual_encoder.compile(\n",
    "    optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db745c-de1d-4173-bf00-c0b8f436c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(f\"Number of examples (caption-image pairs): {train_example_count}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Steps per epoch: {int(np.ceil(train_example_count / batch_size))}\")\n",
    "train_dataset = get_dataset(os.path.join(tfrecords_dir, \"train-*.tfrecord\"), batch_size)\n",
    "valid_dataset = get_dataset(os.path.join(tfrecords_dir, \"valid-*.tfrecord\"), batch_size)\n",
    "# Create a learning rate scheduler callback.\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\", factor=0.2, patience=3\n",
    ")\n",
    "# Create an early stopping callback.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "history = dual_encoder.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    callbacks=[reduce_lr, early_stopping],\n",
    ")\n",
    "print(\"Training completed. Saving vision and text encoders...\")\n",
    "vision_encoder.save(\"vision_encoder\")\n",
    "text_encoder.save(\"text_encoder\")\n",
    "print(\"Models are saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ec4fb2-5886-416c-9dd1-37361c73e50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d884ad9-42bf-4d17-b712-3d3d84e5a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"train\", \"valid\"], loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd870db-989d-4542-b66e-81a0df474ce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-cpu.2-11:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
